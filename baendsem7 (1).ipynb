{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13882310,"sourceType":"datasetVersion","datasetId":8844699}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"NOTEBOOK 7: HYPERPARAMETER TUNING - 5 MODELS, 3 SETS\")\nprint(\"=\"*80)\n\n# ========== CONFIG ==========\nTRAIN_DIR = '/kaggle/input/split-dataset/train'\nVAL_DIR = '/kaggle/input/split-dataset/val'\nTEST_DIR = '/kaggle/input/split-dataset/test'\nOUTPUT_DIR = '/kaggle/working'\nIMG_SIZE = 224\nNUM_EPOCHS = 10  # reduced for quick tuning; increase if needed\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n✓ Device: {DEVICE}\")\nprint(f\"✓ Image size: {IMG_SIZE}\")\nprint(f\"✓ Epochs per run: {NUM_EPOCHS}\")\n\n# ========== CUSTOM DATASET ==========\nclass OCTDataset(Dataset):\n    def __init__(self, root_dir, img_size=224):\n        self.img_size = img_size\n        self.images = []\n        self.labels = []\n        self.class_names = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n        \n        for label, cls_name in enumerate(self.class_names):\n            cls_path = os.path.join(root_dir, cls_name)\n            for img_file in os.listdir(cls_path):\n                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    self.images.append(os.path.join(cls_path, img_file))\n                    self.labels.append(label)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        if img is None:\n            img = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n        img = cv2.resize(img, (self.img_size, self.img_size))\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        img_norm = img_rgb.astype(np.float32) / 255.0\n        img_norm = (img_norm - 0.5) / 0.5\n        img_tensor = torch.from_numpy(img_norm).permute(2, 0, 1)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return img_tensor, label\n\n# Load datasets once\ntrain_dataset = OCTDataset(TRAIN_DIR, IMG_SIZE)\nval_dataset = OCTDataset(VAL_DIR, IMG_SIZE)\ntest_dataset = OCTDataset(TEST_DIR, IMG_SIZE)\n\nprint(f\"\\n✓ Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n\n# ========== CUSTOM HEAD FOR CNNS ==========\nclass CustomHead(nn.Module):\n    def __init__(self, in_features, num_classes=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n    def forward(self, x):\n        return self.fc(x)\n\n# ========== HYPERPARAMETER SETS ==========\nhyperparam_sets = [\n    {\n        \"name\": \"Set1_Adam_lr1e-3_bs32\",\n        \"optimizer\": \"adam\",\n        \"lr\": 1e-3,\n        \"batch_size\": 32,\n        \"weight_decay\": 1e-4\n    },\n    {\n        \"name\": \"Set2_Adam_lr5e-4_bs16\",\n        \"optimizer\": \"adam\",\n        \"lr\": 5e-4,\n        \"batch_size\": 16,\n        \"weight_decay\": 1e-4\n    },\n    {\n        \"name\": \"Set3_SGD_lr1e-2_bs32\",\n        \"optimizer\": \"sgd\",\n        \"lr\": 1e-2,\n        \"batch_size\": 32,\n        \"weight_decay\": 5e-4,\n        \"momentum\": 0.9\n    }\n]\n\nprint(\"\\n✓ Setup complete\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:24:46.506768Z","iopub.execute_input":"2025-11-27T04:24:46.507432Z","iopub.status.idle":"2025-11-27T04:24:46.564760Z","shell.execute_reply.started":"2025-11-27T04:24:46.507404Z","shell.execute_reply":"2025-11-27T04:24:46.564115Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nNOTEBOOK 7: HYPERPARAMETER TUNING - 5 MODELS, 3 SETS\n================================================================================\n\n✓ Device: cuda\n✓ Image size: 224\n✓ Epochs per run: 10\n\n✓ Train: 15967, Val: 2000, Test: 2002\n\n✓ Setup complete\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"CELL 1: RESNET50 HYPERPARAMETER TUNING\")\nprint(\"=\"*80)\n\nMODEL_NAME = \"resnet50\"\nresults_resnet50 = []\n\nfor cfg in hyperparam_sets:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {MODEL_NAME} - {cfg['name']}\")\n    print(f\"{'='*60}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    \n    # Build model\n    model = timm.create_model('resnet50', pretrained=False, num_classes=4)\n    model.fc = CustomHead(2048, num_classes=4)\n    model = model.to(DEVICE)\n    model.train()\n    \n    # Build optimizer\n    if cfg['optimizer'] == 'adam':\n        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n    else:  # SGD\n        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=cfg['momentum'], \n                             weight_decay=cfg['weight_decay'])\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_acc = 0.0\n    best_state = None\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n    \n    # Training loop\n    for epoch in range(NUM_EPOCHS):\n        # --- Train ---\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (train)\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * x.size(0)\n            preds = out.argmax(1)\n            train_correct += (preds == y).sum().item()\n            train_total += y.size(0)\n        \n        train_loss /= train_total\n        train_acc = train_correct / train_total\n        \n        # --- Validate ---\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (val)\", leave=False):\n                x, y = x.to(DEVICE), y.to(DEVICE)\n                out = model(x)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1)\n                val_correct += (preds == y).sum().item()\n                val_total += y.size(0)\n        \n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        \n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        \n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_state = model.state_dict().copy()\n    \n    # Load best model\n    model.load_state_dict(best_state)\n    \n    # --- Test Evaluation ---\n    model.eval()\n    test_correct = 0\n    test_total = 0\n    \n    with torch.no_grad():\n        for x, y in tqdm(test_loader, desc=\"Testing\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            out = model(x)\n            preds = out.argmax(1)\n            test_correct += (preds == y).sum().item()\n            test_total += y.size(0)\n    \n    test_acc = test_correct / test_total\n    \n    results_resnet50.append({\n        \"Model\": \"ResNet50\",\n        \"Hyperparam Set\": cfg['name'],\n        \"LR\": cfg['lr'],\n        \"Batch Size\": cfg['batch_size'],\n        \"Optimizer\": cfg['optimizer'],\n        \"Best Val Acc\": f\"{best_val_acc:.4f}\",\n        \"Test Acc\": f\"{test_acc:.4f}\"\n    })\n    \n    print(f\"\\n✓ {cfg['name']} completed | Best Val Acc: {best_val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n# Display results\nprint(\"\\n\" + \"=\"*80)\nprint(\"RESNET50 RESULTS\")\nprint(\"=\"*80)\ndf_resnet50 = pd.DataFrame(results_resnet50)\nprint(df_resnet50.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T04:24:52.390712Z","iopub.execute_input":"2025-11-27T04:24:52.391005Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nCELL 1: RESNET50 HYPERPARAMETER TUNING\n================================================================================\n\n============================================================\nRunning: resnet50 - Set1_Adam_lr1e-3_bs32\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Train Loss: 0.9978, Val Loss: 0.9351, Val Acc: 0.6070\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 | Train Loss: 0.8714, Val Loss: 1.1127, Val Acc: 0.5700\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 | Train Loss: 0.8275, Val Loss: 0.8615, Val Acc: 0.6555\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 | Train Loss: 0.7863, Val Loss: 0.9794, Val Acc: 0.6460\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 | Train Loss: 0.7532, Val Loss: 0.8176, Val Acc: 0.6785\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 | Train Loss: 0.7205, Val Loss: 0.8809, Val Acc: 0.6490\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 | Train Loss: 0.6872, Val Loss: 0.7083, Val Acc: 0.7190\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 | Train Loss: 0.6713, Val Loss: 0.6719, Val Acc: 0.7270\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 | Train Loss: 0.6549, Val Loss: 0.6344, Val Acc: 0.7425\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 | Train Loss: 0.6461, Val Loss: 0.6165, Val Acc: 0.7455\n","output_type":"stream"},{"name":"stderr","text":"                                                        \r","output_type":"stream"},{"name":"stdout","text":"\n✓ Set1_Adam_lr1e-3_bs32 completed | Best Val Acc: 0.7455, Test Acc: 0.7248\n\n============================================================\nRunning: resnet50 - Set2_Adam_lr5e-4_bs16\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Train Loss: 1.0252, Val Loss: 0.8889, Val Acc: 0.6590\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 | Train Loss: 0.8887, Val Loss: 0.8621, Val Acc: 0.6480\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 | Train Loss: 0.8423, Val Loss: 0.8714, Val Acc: 0.6455\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 | Train Loss: 0.7975, Val Loss: 0.7502, Val Acc: 0.6935\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 | Train Loss: 0.7511, Val Loss: 0.9374, Val Acc: 0.5630\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 | Train Loss: 0.7140, Val Loss: 0.6336, Val Acc: 0.7455\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 | Train Loss: 0.6941, Val Loss: 1.2693, Val Acc: 0.5075\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 | Train Loss: 0.6761, Val Loss: 0.6662, Val Acc: 0.7285\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 | Train Loss: 0.6629, Val Loss: 0.6081, Val Acc: 0.7480\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 | Train Loss: 0.6485, Val Loss: 0.7103, Val Acc: 0.6965\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"\n✓ Set2_Adam_lr5e-4_bs16 completed | Best Val Acc: 0.7480, Test Acc: 0.6983\n\n============================================================\nRunning: resnet50 - Set3_SGD_lr1e-2_bs32\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Train Loss: 1.1679, Val Loss: 1.1934, Val Acc: 0.5420\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 | Train Loss: 0.9764, Val Loss: 1.9266, Val Acc: 0.4275\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 | Train Loss: 0.9170, Val Loss: 1.0837, Val Acc: 0.5420\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 | Train Loss: 0.8829, Val Loss: 1.1164, Val Acc: 0.5250\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 | Train Loss: 0.8328, Val Loss: 1.2597, Val Acc: 0.5310\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 (train):  97%|█████████▋| 485/499 [03:20<00:05,  2.37it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"CELL 2: MOBILENETV2 HYPERPARAMETER TUNING\")\nprint(\"=\"*80)\n\nMODEL_NAME = \"mobilenetv2\"\nresults_mobilenetv2 = []\n\nfor cfg in hyperparam_sets:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {MODEL_NAME} - {cfg['name']}\")\n    print(f\"{'='*60}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    \n    # Build model\n    model = timm.create_model('mobilenetv2_100', pretrained=False, num_classes=4)\n    model.classifier = CustomHead(1280, num_classes=4)\n    model = model.to(DEVICE)\n    model.train()\n    \n    # Build optimizer\n    if cfg['optimizer'] == 'adam':\n        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n    else:  # SGD\n        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=cfg['momentum'], \n                             weight_decay=cfg['weight_decay'])\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_acc = 0.0\n    best_state = None\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n    \n    # Training loop\n    for epoch in range(NUM_EPOCHS):\n        # --- Train ---\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (train)\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * x.size(0)\n            preds = out.argmax(1)\n            train_correct += (preds == y).sum().item()\n            train_total += y.size(0)\n        \n        train_loss /= train_total\n        train_acc = train_correct / train_total\n        \n        # --- Validate ---\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (val)\", leave=False):\n                x, y = x.to(DEVICE), y.to(DEVICE)\n                out = model(x)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1)\n                val_correct += (preds == y).sum().item()\n                val_total += y.size(0)\n        \n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        \n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        \n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_state = model.state_dict().copy()\n    \n    # Load best model\n    model.load_state_dict(best_state)\n    \n    # --- Test Evaluation ---\n    model.eval()\n    test_correct = 0\n    test_total = 0\n    \n    with torch.no_grad():\n        for x, y in tqdm(test_loader, desc=\"Testing\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            out = model(x)\n            preds = out.argmax(1)\n            test_correct += (preds == y).sum().item()\n            test_total += y.size(0)\n    \n    test_acc = test_correct / test_total\n    \n    results_mobilenetv2.append({\n        \"Model\": \"MobileNetV2\",\n        \"Hyperparam Set\": cfg['name'],\n        \"LR\": cfg['lr'],\n        \"Batch Size\": cfg['batch_size'],\n        \"Optimizer\": cfg['optimizer'],\n        \"Best Val Acc\": f\"{best_val_acc:.4f}\",\n        \"Test Acc\": f\"{test_acc:.4f}\"\n    })\n    \n    print(f\"\\n✓ {cfg['name']} completed | Best Val Acc: {best_val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n# Display results\nprint(\"\\n\" + \"=\"*80)\nprint(\"MOBILENETV2 RESULTS\")\nprint(\"=\"*80)\ndf_mobilenetv2 = pd.DataFrame(results_mobilenetv2)\nprint(df_mobilenetv2.to_string(index=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"CELL 3: EFFICIENTNETB0 HYPERPARAMETER TUNING\")\nprint(\"=\"*80)\n\nMODEL_NAME = \"efficientnetb0\"\nresults_efficientnetb0 = []\n\nfor cfg in hyperparam_sets:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {MODEL_NAME} - {cfg['name']}\")\n    print(f\"{'='*60}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    \n    # Build model\n    model = timm.create_model('efficientnet_b0', pretrained=False, num_classes=4)\n    model.classifier = CustomHead(1280, num_classes=4)\n    model = model.to(DEVICE)\n    model.train()\n    \n    # Build optimizer\n    if cfg['optimizer'] == 'adam':\n        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n    else:  # SGD\n        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=cfg['momentum'], \n                             weight_decay=cfg['weight_decay'])\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_acc = 0.0\n    best_state = None\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n    \n    # Training loop\n    for epoch in range(NUM_EPOCHS):\n        # --- Train ---\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (train)\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * x.size(0)\n            preds = out.argmax(1)\n            train_correct += (preds == y).sum().item()\n            train_total += y.size(0)\n        \n        train_loss /= train_total\n        train_acc = train_correct / train_total\n        \n        # --- Validate ---\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (val)\", leave=False):\n                x, y = x.to(DEVICE), y.to(DEVICE)\n                out = model(x)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1)\n                val_correct += (preds == y).sum().item()\n                val_total += y.size(0)\n        \n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        \n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        \n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_state = model.state_dict().copy()\n    \n    # Load best model\n    model.load_state_dict(best_state)\n    \n    # --- Test Evaluation ---\n    model.eval()\n    test_correct = 0\n    test_total = 0\n    \n    with torch.no_grad():\n        for x, y in tqdm(test_loader, desc=\"Testing\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            out = model(x)\n            preds = out.argmax(1)\n            test_correct += (preds == y).sum().item()\n            test_total += y.size(0)\n    \n    test_acc = test_correct / test_total\n    \n    results_efficientnetb0.append({\n        \"Model\": \"EfficientNetB0\",\n        \"Hyperparam Set\": cfg['name'],\n        \"LR\": cfg['lr'],\n        \"Batch Size\": cfg['batch_size'],\n        \"Optimizer\": cfg['optimizer'],\n        \"Best Val Acc\": f\"{best_val_acc:.4f}\",\n        \"Test Acc\": f\"{test_acc:.4f}\"\n    })\n    \n    print(f\"\\n✓ {cfg['name']} completed | Best Val Acc: {best_val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n# Display results\nprint(\"\\n\" + \"=\"*80)\nprint(\"EFFICIENTNETB0 RESULTS\")\nprint(\"=\"*80)\ndf_efficientnetb0 = pd.DataFrame(results_efficientnetb0)\nprint(df_efficientnetb0.to_string(index=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"CELL 4: SWIN HYPERPARAMETER TUNING\")\nprint(\"=\"*80)\n\nMODEL_NAME = \"swin\"\nresults_swin = []\n\nfor cfg in hyperparam_sets:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {MODEL_NAME} - {cfg['name']}\")\n    print(f\"{'='*60}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    \n    # Build model\n    model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4)\n    model = model.to(DEVICE)\n    model.train()\n    \n    # Build optimizer\n    if cfg['optimizer'] == 'adam':\n        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n    else:  # SGD\n        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=cfg['momentum'], \n                             weight_decay=cfg['weight_decay'])\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_acc = 0.0\n    best_state = None\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n    \n    # Training loop\n    for epoch in range(NUM_EPOCHS):\n        # --- Train ---\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (train)\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * x.size(0)\n            preds = out.argmax(1)\n            train_correct += (preds == y).sum().item()\n            train_total += y.size(0)\n        \n        train_loss /= train_total\n        train_acc = train_correct / train_total\n        \n        # --- Validate ---\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (val)\", leave=False):\n                x, y = x.to(DEVICE), y.to(DEVICE)\n                out = model(x)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1)\n                val_correct += (preds == y).sum().item()\n                val_total += y.size(0)\n        \n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        \n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        \n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_state = model.state_dict().copy()\n    \n    # Load best model\n    model.load_state_dict(best_state)\n    \n    # --- Test Evaluation ---\n    model.eval()\n    test_correct = 0\n    test_total = 0\n    \n    with torch.no_grad():\n        for x, y in tqdm(test_loader, desc=\"Testing\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            out = model(x)\n            preds = out.argmax(1)\n            test_correct += (preds == y).sum().item()\n            test_total += y.size(0)\n    \n    test_acc = test_correct / test_total\n    \n    results_swin.append({\n        \"Model\": \"Swin\",\n        \"Hyperparam Set\": cfg['name'],\n        \"LR\": cfg['lr'],\n        \"Batch Size\": cfg['batch_size'],\n        \"Optimizer\": cfg['optimizer'],\n        \"Best Val Acc\": f\"{best_val_acc:.4f}\",\n        \"Test Acc\": f\"{test_acc:.4f}\"\n    })\n    \n    print(f\"\\n✓ {cfg['name']} completed | Best Val Acc: {best_val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n# Display results\nprint(\"\\n\" + \"=\"*80)\nprint(\"SWIN RESULTS\")\nprint(\"=\"*80)\ndf_swin = pd.DataFrame(results_swin)\nprint(df_swin.to_string(index=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"CELL 5: VIT HYPERPARAMETER TUNING\")\nprint(\"=\"*80)\n\nMODEL_NAME = \"vit\"\nresults_vit = []\n\nfor cfg in hyperparam_sets:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {MODEL_NAME} - {cfg['name']}\")\n    print(f\"{'='*60}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=cfg['batch_size'], shuffle=False)\n    \n    # Build model\n    model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4)\n    model = model.to(DEVICE)\n    model.train()\n    \n    # Build optimizer\n    if cfg['optimizer'] == 'adam':\n        optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n    else:  # SGD\n        optimizer = optim.SGD(model.parameters(), lr=cfg['lr'], momentum=cfg['momentum'], \n                             weight_decay=cfg['weight_decay'])\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_acc = 0.0\n    best_state = None\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n    \n    # Training loop\n    for epoch in range(NUM_EPOCHS):\n        # --- Train ---\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (train)\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * x.size(0)\n            preds = out.argmax(1)\n            train_correct += (preds == y).sum().item()\n            train_total += y.size(0)\n        \n        train_loss /= train_total\n        train_acc = train_correct / train_total\n        \n        # --- Validate ---\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} (val)\", leave=False):\n                x, y = x.to(DEVICE), y.to(DEVICE)\n                out = model(x)\n                loss = criterion(out, y)\n                val_loss += loss.item() * x.size(0)\n                preds = out.argmax(1)\n                val_correct += (preds == y).sum().item()\n                val_total += y.size(0)\n        \n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        \n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        \n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_state = model.state_dict().copy()\n    \n    # Load best model\n    model.load_state_dict(best_state)\n    \n    # --- Test Evaluation ---\n    model.eval()\n    test_correct = 0\n    test_total = 0\n    \n    with torch.no_grad():\n        for x, y in tqdm(test_loader, desc=\"Testing\", leave=False):\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            out = model(x)\n            preds = out.argmax(1)\n            test_correct += (preds == y).sum().item()\n            test_total += y.size(0)\n    \n    test_acc = test_correct / test_total\n    \n    results_vit.append({\n        \"Model\": \"ViT\",\n        \"Hyperparam Set\": cfg['name'],\n        \"LR\": cfg['lr'],\n        \"Batch Size\": cfg['batch_size'],\n        \"Optimizer\": cfg['optimizer'],\n        \"Best Val Acc\": f\"{best_val_acc:.4f}\",\n        \"Test Acc\": f\"{test_acc:.4f}\"\n    })\n    \n    print(f\"\\n✓ {cfg['name']} completed | Best Val Acc: {best_val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n\n# Display results\nprint(\"\\n\" + \"=\"*80)\nprint(\"VIT RESULTS\")\nprint(\"=\"*80)\ndf_vit = pd.DataFrame(results_vit)\nprint(df_vit.to_string(index=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"COMBINED HYPERPARAMETER TUNING SUMMARY\")\nprint(\"=\"*80)\n\n# Combine all results\nall_results = results_resnet50 + results_mobilenetv2 + results_efficientnetb0 + results_swin + results_vit\ndf_all = pd.DataFrame(all_results)\n\nprint(\"\\nAll Models - All Hyperparameter Sets:\")\nprint(df_all.to_string(index=False))\n\n# Save to CSV\ncsv_path = os.path.join(OUTPUT_DIR, 'hyperparameter_tuning_results.csv')\ndf_all.to_csv(csv_path, index=False)\nprint(f\"\\n✓ Results saved to: {csv_path}\")\n\n# Best per model\nprint(\"\\n\" + \"=\"*80)\nprint(\"BEST HYPERPARAMETER SET PER MODEL\")\nprint(\"=\"*80)\nfor model in [\"ResNet50\", \"MobileNetV2\", \"EfficientNetB0\", \"Swin\", \"ViT\"]:\n    model_data = df_all[df_all[\"Model\"] == model]\n    best_idx = model_data['Test Acc'].str.replace(\"0.\", \"\").astype(float).idxmax()\n    best_row = df_all.loc[best_idx]\n    print(f\"\\n{model}:\")\n    print(f\"  Best Set: {best_row['Hyperparam Set']}\")\n    print(f\"  Val Acc: {best_row['Best Val Acc']}, Test Acc: {best_row['Test Acc']}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"HYPERPARAMETER TUNING COMPLETE\")\nprint(\"=\"*80)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}