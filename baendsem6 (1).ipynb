{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13882310,"sourceType":"datasetVersion","datasetId":8844699},{"sourceId":13886075,"sourceType":"datasetVersion","datasetId":8847063}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport timm\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"NOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS\")\nprint(\"=\"*80)\n\n# ========== CONFIG ==========\nTEST_DIR = '/kaggle/input/split-dataset/test'\nWEIGHTS_DIR = '/kaggle/input/weight/weights'\nOUTPUT_DIR = '/kaggle/working'\nIMG_SIZE = 224\nNUM_IMAGES_PER_CLASS = 200\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n✓ Device: {DEVICE}\")\nprint(f\"✓ Test dataset: {TEST_DIR}\")\nprint(f\"✓ Weights directory: {WEIGHTS_DIR}\")\nprint(f\"✓ Output directory: {OUTPUT_DIR}\")\n\n# ========== 1. LOAD TEST IMAGES ==========\nprint(\"\\n[1/5] Loading test images...\")\n\nclass_names = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\ntest_images = {cls: [] for cls in class_names}\n\nfor cls in class_names:\n    cls_path = os.path.join(TEST_DIR, cls)\n    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    sampled = random.sample(images, min(NUM_IMAGES_PER_CLASS, len(images)))\n    test_images[cls] = [os.path.join(cls_path, img) for img in sampled]\n\nfor cls in class_names:\n    print(f\"  {cls}: {len(test_images[cls])}\")\ntotal_images = sum(len(v) for v in test_images.values())\nprint(f\"✓ Total test images: {total_images}\")\n\n# ========== 2. PREPROCESS ==========\nprint(\"\\n[2/5] Setting up preprocessing...\")\n\ndef preprocess_image(img_path):\n    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    image_norm = image_rgb.astype(np.float32) / 255.0\n    image_norm = (image_norm - 0.5) / 0.5\n    img_tensor = torch.from_numpy(image_norm).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n    img_tensor.requires_grad_(True)  # Enable grad for CAM backward\n    return img_tensor, image_rgb\n\nprint(\"✓ Preprocessing ready\")\n\n# ========== 3. CUSTOM CLASSIFIER HEAD (for CNN models) ==========\nclass CustomHead(nn.Module):\n    def __init__(self, in_features, num_classes=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\n# ========== 4. LOAD MODELS ==========\nprint(\"\\n[3/5] Loading trained models...\")\n\nmodel_configs = {\n    'resnet50': {\n        'model_fn': lambda: timm.create_model('resnet50', pretrained=False, num_classes=4),\n        'weight_file': 'resnet50_best_80_10_10.pth',\n        'target_layer': 'layer4[-1]',\n        'has_custom_head': True,\n        'num_features': 2048\n    },\n    'mobilenetv2': {\n        'model_fn': lambda: timm.create_model('mobilenetv2_100', pretrained=False, num_classes=4),\n        'weight_file': 'mobilenetv2_best_80_10_10.pth',\n        'target_layer': 'blocks[-1]',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'efficientnetb0': {\n        'model_fn': lambda: timm.create_model('efficientnet_b0', pretrained=False, num_classes=4),\n        'weight_file': 'efficientnetb0_best_80_10_10.pth',\n        'target_layer': 'blocks[-1]',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'vit': {\n        'model_fn': lambda: timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4),\n        'weight_file': 'vit_best_80_10_10.pth',\n        'target_layer': 'blocks[-1]',\n        'has_custom_head': False\n    },\n    'swin': {\n        'model_fn': lambda: timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4),\n        'weight_file': 'swin_best_80_10_10.pth',\n        'target_layer': 'layers[-1]',\n        'has_custom_head': False\n    }\n}\n\nmodels = {}\nfor model_name, cfg in model_configs.items():\n    try:\n        model = cfg['model_fn']()\n        \n        if cfg['has_custom_head']:\n            model.fc = CustomHead(cfg['num_features'], num_classes=4)\n        \n        weight_path = os.path.join(WEIGHTS_DIR, cfg['weight_file'])\n        if os.path.exists(weight_path):\n            model.load_state_dict(torch.load(weight_path, map_location=DEVICE), strict=False)\n            model.to(DEVICE)\n            model.eval()\n            models[model_name] = model\n            print(f\"✓ {model_name.upper()} loaded from {weight_path}\")\n        else:\n            print(f\"⚠ {model_name.upper()} weights not found at {weight_path}\")\n    except Exception as e:\n        print(f\"✗ Error loading {model_name}: {e}\")\n\nprint(f\"\\n✓ Total models loaded: {len(models)}\")\n\n# ========== 5. GRAD-CAM ==========\nprint(\"\\n[4/5] Implementing Grad-CAM...\")\n\nclass GradCAM:\n    def __init__(self, model, target_layer_name):\n        self.model = model\n        self.target_layer_name = target_layer_name\n        self.gradients = None\n        self.activations = None\n        self.hooks = []\n        self._register_hooks()\n    \n    def _register_hooks(self):\n        def forward_hook(module, inp, out):\n            self.activations = out\n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0]\n        for name, module in self.model.named_modules():\n            if self.target_layer_name in name or name.endswith(self.target_layer_name):\n                self.hooks.append(module.register_forward_hook(forward_hook))\n                self.hooks.append(module.register_full_backward_hook(backward_hook))\n                break\n    \n    def generate_cam(self, input_tensor, class_idx=None):\n        output = self.model(input_tensor)\n        if class_idx is None:\n            class_idx = output.argmax(dim=1).item()\n        self.model.zero_grad()\n        score = output[0, class_idx]\n        score.backward(retain_graph=True)\n\n        grads = self.gradients.detach().cpu().numpy()[0]      # (C,H,W)\n        acts = self.activations.detach().cpu().numpy()[0]     # (C,H,W)\n        weights = grads.mean(axis=(1, 2))                     # (C,)\n        cam = np.zeros(acts.shape[1:], dtype=np.float32)\n        for c, w in enumerate(weights):\n            cam += w * acts[c]\n        cam = np.maximum(cam, 0)\n        cam = cam / (cam.max() + 1e-8)\n        return cam, class_idx\n    \n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n\nprint(\"✓ Grad-CAM ready\")\n\n# ========== 6. GENERATE CAMs ==========\nprint(\"\\n[5/5] Generating CAMs...\")\n\ncam_output_dir = os.path.join(OUTPUT_DIR, 'cam_visualizations')\nos.makedirs(cam_output_dir, exist_ok=True)\n\nall_cams = {}\n\nif len(models) == 0:\n    print(\"✗ No models loaded. Cannot generate CAMs.\")\nelse:\n    for model_name, model in tqdm(models.items(), desc=\"Processing models\"):\n        target_layer = model_configs[model_name]['target_layer']\n        cam_engine = GradCAM(model, target_layer)\n        all_cams[model_name] = {}\n        \n        for cls_name in class_names:\n            all_cams[model_name][cls_name] = []\n            for img_idx, img_path in enumerate(test_images[cls_name]):\n                try:\n                    img_tensor, img_rgb = preprocess_image(img_path)\n                    cam, pred_class = cam_engine.generate_cam(img_tensor)\n                    cam_resized = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n                    cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                    cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                    img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                    overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                    save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                    cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                    all_cams[model_name][cls_name].append({\n                        'path': img_path,\n                        'overlay': overlay,\n                        'pred_class': pred_class\n                    })\n                except Exception as e:\n                    pass  # Silently skip errors\n        \n        cam_engine.remove_hooks()\n        print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n\n    print(f\"\\n✓ All CAM overlays saved to: {cam_output_dir}\")\n\n    # ========== 7. CREATE COMPARISON GRIDS ==========\n    print(\"\\n[6/6] Creating comparison grids...\")\n\n    for cls_name in class_names:\n        fig, axes = plt.subplots(1, len(models) + 1, figsize=(24, 4))\n        first_img_path = test_images[cls_name][0]\n        _, img_rgb = preprocess_image(first_img_path)\n        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n        axes[0].imshow(img_rgb_uint8, cmap='gray')\n        axes[0].set_title(f'{cls_name} - Original', fontsize=11, fontweight='bold')\n        axes[0].axis('off')\n        for i, (model_name, _) in enumerate(models.items()):\n            if len(all_cams[model_name][cls_name]) > 0:\n                overlay = all_cams[model_name][cls_name][0]['overlay']\n                axes[i+1].imshow(overlay)\n                axes[i+1].set_title(model_name.upper(), fontsize=11, fontweight='bold')\n            axes[i+1].axis('off')\n        plt.tight_layout()\n        plt.savefig(os.path.join(cam_output_dir, f'comparison_grid_{cls_name}.png'), dpi=300)\n        plt.close()\n        print(f\"✓ Saved comparison grid for {cls_name}\")\n\n# ========== 8. SUMMARY ==========\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: GRAD-CAM ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n\nif len(models) > 0:\n    print(f\"\\nModels processed: {list(models.keys())}\")\n    print(f\"Classes analyzed: {class_names}\")\n    print(f\"Images per class: {NUM_IMAGES_PER_CLASS}\")\n    total_cams = sum(len(all_cams[m][c]) for m in models.keys() for c in class_names)\n    print(f\"Total CAM visualizations: {total_cams}\")\n    print(f\"\\nOutput: {cam_output_dir}\")\n    print(f\"Files: individual cam_<model>_lass>_<idx>.png + comparison_grid_lass>.png\")\n    print(\"\\n✓ All CAM visualizations saved successfully!\")\nelse:\n    print(\"\\n✗ No models loaded. Check weights.\")\n\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T21:48:57.222630Z","iopub.execute_input":"2025-11-26T21:48:57.222956Z","iopub.status.idle":"2025-11-26T21:50:56.838879Z","shell.execute_reply.started":"2025-11-26T21:48:57.222933Z","shell.execute_reply":"2025-11-26T21:50:56.838151Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nNOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS\n================================================================================\n\n✓ Device: cuda\n✓ Test dataset: /kaggle/input/split-dataset/test\n✓ Weights directory: /kaggle/input/weight/weights\n✓ Output directory: /kaggle/working\n\n[1/5] Loading test images...\n  CNV: 200\n  DME: 200\n  DRUSEN: 200\n  NORMAL: 200\n✓ Total test images: 800\n\n[2/5] Setting up preprocessing...\n✓ Preprocessing ready\n\n[3/5] Loading trained models...\n✓ RESNET50 loaded from /kaggle/input/weight/weights/resnet50_best_80_10_10.pth\n✓ MOBILENETV2 loaded from /kaggle/input/weight/weights/mobilenetv2_best_80_10_10.pth\n✓ EFFICIENTNETB0 loaded from /kaggle/input/weight/weights/efficientnetb0_best_80_10_10.pth\n✓ VIT loaded from /kaggle/input/weight/weights/vit_best_80_10_10.pth\n✓ SWIN loaded from /kaggle/input/weight/weights/swin_best_80_10_10.pth\n\n✓ Total models loaded: 5\n\n[4/5] Implementing Grad-CAM...\n✓ Grad-CAM ready\n\n[5/5] Generating CAMs...\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  20%|██        | 1/5 [00:17<01:10, 17.65s/it]","output_type":"stream"},{"name":"stdout","text":"✓ RESNET50: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  40%|████      | 2/5 [00:32<00:47, 15.96s/it]","output_type":"stream"},{"name":"stdout","text":"✓ MOBILENETV2: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  60%|██████    | 3/5 [00:53<00:36, 18.46s/it]","output_type":"stream"},{"name":"stdout","text":"✓ EFFICIENTNETB0: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  80%|████████  | 4/5 [01:26<00:23, 23.95s/it]","output_type":"stream"},{"name":"stdout","text":"✓ VIT: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models: 100%|██████████| 5/5 [01:54<00:00, 22.89s/it]","output_type":"stream"},{"name":"stdout","text":"✓ SWIN: 0 CAMs per class\n\n✓ All CAM overlays saved to: /kaggle/working/cam_visualizations\n\n[6/6] Creating comparison grids...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved comparison grid for CNV\n✓ Saved comparison grid for DME\n✓ Saved comparison grid for DRUSEN\n✓ Saved comparison grid for NORMAL\n\n================================================================================\nSUMMARY: GRAD-CAM ANALYSIS COMPLETE\n================================================================================\n\nModels processed: ['resnet50', 'mobilenetv2', 'efficientnetb0', 'vit', 'swin']\nClasses analyzed: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\nImages per class: 200\nTotal CAM visualizations: 0\n\nOutput: /kaggle/working/cam_visualizations\nFiles: individual cam_<model>_lass>_<idx>.png + comparison_grid_lass>.png\n\n✓ All CAM visualizations saved successfully!\n================================================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Example for resnet50\nfor name, module in models['resnet50'].named_modules():\n    print(name)\n\n# Similarly for other models, look for last conv layers, e.g.\n# ResNet50 typically \"layer4[-1].conv3\" or \"layer4[-1]\"\n# EfficientNet might be \"blocks[-1]\"\n# Others similarly.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T21:52:21.333208Z","iopub.execute_input":"2025-11-26T21:52:21.333946Z","iopub.status.idle":"2025-11-26T21:52:21.340319Z","shell.execute_reply.started":"2025-11-26T21:52:21.333919Z","shell.execute_reply":"2025-11-26T21:52:21.339652Z"}},"outputs":[{"name":"stdout","text":"\nconv1\nbn1\nact1\nmaxpool\nlayer1\nlayer1.0\nlayer1.0.conv1\nlayer1.0.bn1\nlayer1.0.act1\nlayer1.0.conv2\nlayer1.0.bn2\nlayer1.0.drop_block\nlayer1.0.act2\nlayer1.0.aa\nlayer1.0.conv3\nlayer1.0.bn3\nlayer1.0.act3\nlayer1.0.downsample\nlayer1.0.downsample.0\nlayer1.0.downsample.1\nlayer1.1\nlayer1.1.conv1\nlayer1.1.bn1\nlayer1.1.act1\nlayer1.1.conv2\nlayer1.1.bn2\nlayer1.1.drop_block\nlayer1.1.act2\nlayer1.1.aa\nlayer1.1.conv3\nlayer1.1.bn3\nlayer1.1.act3\nlayer1.2\nlayer1.2.conv1\nlayer1.2.bn1\nlayer1.2.act1\nlayer1.2.conv2\nlayer1.2.bn2\nlayer1.2.drop_block\nlayer1.2.act2\nlayer1.2.aa\nlayer1.2.conv3\nlayer1.2.bn3\nlayer1.2.act3\nlayer2\nlayer2.0\nlayer2.0.conv1\nlayer2.0.bn1\nlayer2.0.act1\nlayer2.0.conv2\nlayer2.0.bn2\nlayer2.0.drop_block\nlayer2.0.act2\nlayer2.0.aa\nlayer2.0.conv3\nlayer2.0.bn3\nlayer2.0.act3\nlayer2.0.downsample\nlayer2.0.downsample.0\nlayer2.0.downsample.1\nlayer2.1\nlayer2.1.conv1\nlayer2.1.bn1\nlayer2.1.act1\nlayer2.1.conv2\nlayer2.1.bn2\nlayer2.1.drop_block\nlayer2.1.act2\nlayer2.1.aa\nlayer2.1.conv3\nlayer2.1.bn3\nlayer2.1.act3\nlayer2.2\nlayer2.2.conv1\nlayer2.2.bn1\nlayer2.2.act1\nlayer2.2.conv2\nlayer2.2.bn2\nlayer2.2.drop_block\nlayer2.2.act2\nlayer2.2.aa\nlayer2.2.conv3\nlayer2.2.bn3\nlayer2.2.act3\nlayer2.3\nlayer2.3.conv1\nlayer2.3.bn1\nlayer2.3.act1\nlayer2.3.conv2\nlayer2.3.bn2\nlayer2.3.drop_block\nlayer2.3.act2\nlayer2.3.aa\nlayer2.3.conv3\nlayer2.3.bn3\nlayer2.3.act3\nlayer3\nlayer3.0\nlayer3.0.conv1\nlayer3.0.bn1\nlayer3.0.act1\nlayer3.0.conv2\nlayer3.0.bn2\nlayer3.0.drop_block\nlayer3.0.act2\nlayer3.0.aa\nlayer3.0.conv3\nlayer3.0.bn3\nlayer3.0.act3\nlayer3.0.downsample\nlayer3.0.downsample.0\nlayer3.0.downsample.1\nlayer3.1\nlayer3.1.conv1\nlayer3.1.bn1\nlayer3.1.act1\nlayer3.1.conv2\nlayer3.1.bn2\nlayer3.1.drop_block\nlayer3.1.act2\nlayer3.1.aa\nlayer3.1.conv3\nlayer3.1.bn3\nlayer3.1.act3\nlayer3.2\nlayer3.2.conv1\nlayer3.2.bn1\nlayer3.2.act1\nlayer3.2.conv2\nlayer3.2.bn2\nlayer3.2.drop_block\nlayer3.2.act2\nlayer3.2.aa\nlayer3.2.conv3\nlayer3.2.bn3\nlayer3.2.act3\nlayer3.3\nlayer3.3.conv1\nlayer3.3.bn1\nlayer3.3.act1\nlayer3.3.conv2\nlayer3.3.bn2\nlayer3.3.drop_block\nlayer3.3.act2\nlayer3.3.aa\nlayer3.3.conv3\nlayer3.3.bn3\nlayer3.3.act3\nlayer3.4\nlayer3.4.conv1\nlayer3.4.bn1\nlayer3.4.act1\nlayer3.4.conv2\nlayer3.4.bn2\nlayer3.4.drop_block\nlayer3.4.act2\nlayer3.4.aa\nlayer3.4.conv3\nlayer3.4.bn3\nlayer3.4.act3\nlayer3.5\nlayer3.5.conv1\nlayer3.5.bn1\nlayer3.5.act1\nlayer3.5.conv2\nlayer3.5.bn2\nlayer3.5.drop_block\nlayer3.5.act2\nlayer3.5.aa\nlayer3.5.conv3\nlayer3.5.bn3\nlayer3.5.act3\nlayer4\nlayer4.0\nlayer4.0.conv1\nlayer4.0.bn1\nlayer4.0.act1\nlayer4.0.conv2\nlayer4.0.bn2\nlayer4.0.drop_block\nlayer4.0.act2\nlayer4.0.aa\nlayer4.0.conv3\nlayer4.0.bn3\nlayer4.0.act3\nlayer4.0.downsample\nlayer4.0.downsample.0\nlayer4.0.downsample.1\nlayer4.1\nlayer4.1.conv1\nlayer4.1.bn1\nlayer4.1.act1\nlayer4.1.conv2\nlayer4.1.bn2\nlayer4.1.drop_block\nlayer4.1.act2\nlayer4.1.aa\nlayer4.1.conv3\nlayer4.1.bn3\nlayer4.1.act3\nlayer4.2\nlayer4.2.conv1\nlayer4.2.bn1\nlayer4.2.act1\nlayer4.2.conv2\nlayer4.2.bn2\nlayer4.2.drop_block\nlayer4.2.act2\nlayer4.2.aa\nlayer4.2.conv3\nlayer4.2.bn3\nlayer4.2.act3\nglobal_pool\nglobal_pool.pool\nglobal_pool.flatten\nfc\nfc.fc\nfc.fc.0\nfc.fc.1\nfc.fc.2\nfc.fc.3\nfc.fc.4\nfc.fc.5\nfc.fc.6\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import timm\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 1) ResNet50\nmodel_resnet = timm.create_model('resnet50', pretrained=False, num_classes=4).to(device)\nprint(\"\\nResNet50 named modules:\")\nfor name, module in model_resnet.named_modules():\n    print(name)\n\n# 2) MobileNetV2\nmodel_mobilenet = timm.create_model('mobilenetv2_100', pretrained=False, num_classes=4).to(device)\nprint(\"\\nMobileNetV2 named modules:\")\nfor name, module in model_mobilenet.named_modules():\n    print(name)\n\n# 3) EfficientNetB0\nmodel_efficientnet = timm.create_model('efficientnet_b0', pretrained=False, num_classes=4).to(device)\nprint(\"\\nEfficientNetB0 named modules:\")\nfor name, module in model_efficientnet.named_modules():\n    print(name)\n\n# 4) ViT Base Patch16 224\nmodel_vit = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4).to(device)\nprint(\"\\nViT Base Patch16_224 named modules:\")\nfor name, module in model_vit.named_modules():\n    print(name)\n\n# 5) Swin Tiny Patch4 Window7 224\nmodel_swin = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4).to(device)\nprint(\"\\nSwin Tiny Patch4 Window7_224 named modules:\")\nfor name, module in model_swin.named_modules():\n    print(name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T21:55:27.857011Z","iopub.execute_input":"2025-11-26T21:55:27.857508Z","iopub.status.idle":"2025-11-26T21:55:30.063344Z","shell.execute_reply.started":"2025-11-26T21:55:27.857487Z","shell.execute_reply":"2025-11-26T21:55:30.062572Z"}},"outputs":[{"name":"stdout","text":"\nResNet50 named modules:\n\nconv1\nbn1\nact1\nmaxpool\nlayer1\nlayer1.0\nlayer1.0.conv1\nlayer1.0.bn1\nlayer1.0.act1\nlayer1.0.conv2\nlayer1.0.bn2\nlayer1.0.drop_block\nlayer1.0.act2\nlayer1.0.aa\nlayer1.0.conv3\nlayer1.0.bn3\nlayer1.0.act3\nlayer1.0.downsample\nlayer1.0.downsample.0\nlayer1.0.downsample.1\nlayer1.1\nlayer1.1.conv1\nlayer1.1.bn1\nlayer1.1.act1\nlayer1.1.conv2\nlayer1.1.bn2\nlayer1.1.drop_block\nlayer1.1.act2\nlayer1.1.aa\nlayer1.1.conv3\nlayer1.1.bn3\nlayer1.1.act3\nlayer1.2\nlayer1.2.conv1\nlayer1.2.bn1\nlayer1.2.act1\nlayer1.2.conv2\nlayer1.2.bn2\nlayer1.2.drop_block\nlayer1.2.act2\nlayer1.2.aa\nlayer1.2.conv3\nlayer1.2.bn3\nlayer1.2.act3\nlayer2\nlayer2.0\nlayer2.0.conv1\nlayer2.0.bn1\nlayer2.0.act1\nlayer2.0.conv2\nlayer2.0.bn2\nlayer2.0.drop_block\nlayer2.0.act2\nlayer2.0.aa\nlayer2.0.conv3\nlayer2.0.bn3\nlayer2.0.act3\nlayer2.0.downsample\nlayer2.0.downsample.0\nlayer2.0.downsample.1\nlayer2.1\nlayer2.1.conv1\nlayer2.1.bn1\nlayer2.1.act1\nlayer2.1.conv2\nlayer2.1.bn2\nlayer2.1.drop_block\nlayer2.1.act2\nlayer2.1.aa\nlayer2.1.conv3\nlayer2.1.bn3\nlayer2.1.act3\nlayer2.2\nlayer2.2.conv1\nlayer2.2.bn1\nlayer2.2.act1\nlayer2.2.conv2\nlayer2.2.bn2\nlayer2.2.drop_block\nlayer2.2.act2\nlayer2.2.aa\nlayer2.2.conv3\nlayer2.2.bn3\nlayer2.2.act3\nlayer2.3\nlayer2.3.conv1\nlayer2.3.bn1\nlayer2.3.act1\nlayer2.3.conv2\nlayer2.3.bn2\nlayer2.3.drop_block\nlayer2.3.act2\nlayer2.3.aa\nlayer2.3.conv3\nlayer2.3.bn3\nlayer2.3.act3\nlayer3\nlayer3.0\nlayer3.0.conv1\nlayer3.0.bn1\nlayer3.0.act1\nlayer3.0.conv2\nlayer3.0.bn2\nlayer3.0.drop_block\nlayer3.0.act2\nlayer3.0.aa\nlayer3.0.conv3\nlayer3.0.bn3\nlayer3.0.act3\nlayer3.0.downsample\nlayer3.0.downsample.0\nlayer3.0.downsample.1\nlayer3.1\nlayer3.1.conv1\nlayer3.1.bn1\nlayer3.1.act1\nlayer3.1.conv2\nlayer3.1.bn2\nlayer3.1.drop_block\nlayer3.1.act2\nlayer3.1.aa\nlayer3.1.conv3\nlayer3.1.bn3\nlayer3.1.act3\nlayer3.2\nlayer3.2.conv1\nlayer3.2.bn1\nlayer3.2.act1\nlayer3.2.conv2\nlayer3.2.bn2\nlayer3.2.drop_block\nlayer3.2.act2\nlayer3.2.aa\nlayer3.2.conv3\nlayer3.2.bn3\nlayer3.2.act3\nlayer3.3\nlayer3.3.conv1\nlayer3.3.bn1\nlayer3.3.act1\nlayer3.3.conv2\nlayer3.3.bn2\nlayer3.3.drop_block\nlayer3.3.act2\nlayer3.3.aa\nlayer3.3.conv3\nlayer3.3.bn3\nlayer3.3.act3\nlayer3.4\nlayer3.4.conv1\nlayer3.4.bn1\nlayer3.4.act1\nlayer3.4.conv2\nlayer3.4.bn2\nlayer3.4.drop_block\nlayer3.4.act2\nlayer3.4.aa\nlayer3.4.conv3\nlayer3.4.bn3\nlayer3.4.act3\nlayer3.5\nlayer3.5.conv1\nlayer3.5.bn1\nlayer3.5.act1\nlayer3.5.conv2\nlayer3.5.bn2\nlayer3.5.drop_block\nlayer3.5.act2\nlayer3.5.aa\nlayer3.5.conv3\nlayer3.5.bn3\nlayer3.5.act3\nlayer4\nlayer4.0\nlayer4.0.conv1\nlayer4.0.bn1\nlayer4.0.act1\nlayer4.0.conv2\nlayer4.0.bn2\nlayer4.0.drop_block\nlayer4.0.act2\nlayer4.0.aa\nlayer4.0.conv3\nlayer4.0.bn3\nlayer4.0.act3\nlayer4.0.downsample\nlayer4.0.downsample.0\nlayer4.0.downsample.1\nlayer4.1\nlayer4.1.conv1\nlayer4.1.bn1\nlayer4.1.act1\nlayer4.1.conv2\nlayer4.1.bn2\nlayer4.1.drop_block\nlayer4.1.act2\nlayer4.1.aa\nlayer4.1.conv3\nlayer4.1.bn3\nlayer4.1.act3\nlayer4.2\nlayer4.2.conv1\nlayer4.2.bn1\nlayer4.2.act1\nlayer4.2.conv2\nlayer4.2.bn2\nlayer4.2.drop_block\nlayer4.2.act2\nlayer4.2.aa\nlayer4.2.conv3\nlayer4.2.bn3\nlayer4.2.act3\nglobal_pool\nglobal_pool.pool\nglobal_pool.flatten\nfc\n\nMobileNetV2 named modules:\n\nconv_stem\nbn1\nbn1.drop\nbn1.act\nblocks\nblocks.0\nblocks.0.0\nblocks.0.0.conv_dw\nblocks.0.0.bn1\nblocks.0.0.bn1.drop\nblocks.0.0.bn1.act\nblocks.0.0.aa\nblocks.0.0.se\nblocks.0.0.conv_pw\nblocks.0.0.bn2\nblocks.0.0.bn2.drop\nblocks.0.0.bn2.act\nblocks.0.0.drop_path\nblocks.1\nblocks.1.0\nblocks.1.0.conv_pw\nblocks.1.0.bn1\nblocks.1.0.bn1.drop\nblocks.1.0.bn1.act\nblocks.1.0.conv_dw\nblocks.1.0.bn2\nblocks.1.0.bn2.drop\nblocks.1.0.bn2.act\nblocks.1.0.aa\nblocks.1.0.se\nblocks.1.0.conv_pwl\nblocks.1.0.bn3\nblocks.1.0.bn3.drop\nblocks.1.0.bn3.act\nblocks.1.0.drop_path\nblocks.1.1\nblocks.1.1.conv_pw\nblocks.1.1.bn1\nblocks.1.1.bn1.drop\nblocks.1.1.bn1.act\nblocks.1.1.conv_dw\nblocks.1.1.bn2\nblocks.1.1.bn2.drop\nblocks.1.1.bn2.act\nblocks.1.1.aa\nblocks.1.1.se\nblocks.1.1.conv_pwl\nblocks.1.1.bn3\nblocks.1.1.bn3.drop\nblocks.1.1.bn3.act\nblocks.1.1.drop_path\nblocks.2\nblocks.2.0\nblocks.2.0.conv_pw\nblocks.2.0.bn1\nblocks.2.0.bn1.drop\nblocks.2.0.bn1.act\nblocks.2.0.conv_dw\nblocks.2.0.bn2\nblocks.2.0.bn2.drop\nblocks.2.0.bn2.act\nblocks.2.0.aa\nblocks.2.0.se\nblocks.2.0.conv_pwl\nblocks.2.0.bn3\nblocks.2.0.bn3.drop\nblocks.2.0.bn3.act\nblocks.2.0.drop_path\nblocks.2.1\nblocks.2.1.conv_pw\nblocks.2.1.bn1\nblocks.2.1.bn1.drop\nblocks.2.1.bn1.act\nblocks.2.1.conv_dw\nblocks.2.1.bn2\nblocks.2.1.bn2.drop\nblocks.2.1.bn2.act\nblocks.2.1.aa\nblocks.2.1.se\nblocks.2.1.conv_pwl\nblocks.2.1.bn3\nblocks.2.1.bn3.drop\nblocks.2.1.bn3.act\nblocks.2.1.drop_path\nblocks.2.2\nblocks.2.2.conv_pw\nblocks.2.2.bn1\nblocks.2.2.bn1.drop\nblocks.2.2.bn1.act\nblocks.2.2.conv_dw\nblocks.2.2.bn2\nblocks.2.2.bn2.drop\nblocks.2.2.bn2.act\nblocks.2.2.aa\nblocks.2.2.se\nblocks.2.2.conv_pwl\nblocks.2.2.bn3\nblocks.2.2.bn3.drop\nblocks.2.2.bn3.act\nblocks.2.2.drop_path\nblocks.3\nblocks.3.0\nblocks.3.0.conv_pw\nblocks.3.0.bn1\nblocks.3.0.bn1.drop\nblocks.3.0.bn1.act\nblocks.3.0.conv_dw\nblocks.3.0.bn2\nblocks.3.0.bn2.drop\nblocks.3.0.bn2.act\nblocks.3.0.aa\nblocks.3.0.se\nblocks.3.0.conv_pwl\nblocks.3.0.bn3\nblocks.3.0.bn3.drop\nblocks.3.0.bn3.act\nblocks.3.0.drop_path\nblocks.3.1\nblocks.3.1.conv_pw\nblocks.3.1.bn1\nblocks.3.1.bn1.drop\nblocks.3.1.bn1.act\nblocks.3.1.conv_dw\nblocks.3.1.bn2\nblocks.3.1.bn2.drop\nblocks.3.1.bn2.act\nblocks.3.1.aa\nblocks.3.1.se\nblocks.3.1.conv_pwl\nblocks.3.1.bn3\nblocks.3.1.bn3.drop\nblocks.3.1.bn3.act\nblocks.3.1.drop_path\nblocks.3.2\nblocks.3.2.conv_pw\nblocks.3.2.bn1\nblocks.3.2.bn1.drop\nblocks.3.2.bn1.act\nblocks.3.2.conv_dw\nblocks.3.2.bn2\nblocks.3.2.bn2.drop\nblocks.3.2.bn2.act\nblocks.3.2.aa\nblocks.3.2.se\nblocks.3.2.conv_pwl\nblocks.3.2.bn3\nblocks.3.2.bn3.drop\nblocks.3.2.bn3.act\nblocks.3.2.drop_path\nblocks.3.3\nblocks.3.3.conv_pw\nblocks.3.3.bn1\nblocks.3.3.bn1.drop\nblocks.3.3.bn1.act\nblocks.3.3.conv_dw\nblocks.3.3.bn2\nblocks.3.3.bn2.drop\nblocks.3.3.bn2.act\nblocks.3.3.aa\nblocks.3.3.se\nblocks.3.3.conv_pwl\nblocks.3.3.bn3\nblocks.3.3.bn3.drop\nblocks.3.3.bn3.act\nblocks.3.3.drop_path\nblocks.4\nblocks.4.0\nblocks.4.0.conv_pw\nblocks.4.0.bn1\nblocks.4.0.bn1.drop\nblocks.4.0.bn1.act\nblocks.4.0.conv_dw\nblocks.4.0.bn2\nblocks.4.0.bn2.drop\nblocks.4.0.bn2.act\nblocks.4.0.aa\nblocks.4.0.se\nblocks.4.0.conv_pwl\nblocks.4.0.bn3\nblocks.4.0.bn3.drop\nblocks.4.0.bn3.act\nblocks.4.0.drop_path\nblocks.4.1\nblocks.4.1.conv_pw\nblocks.4.1.bn1\nblocks.4.1.bn1.drop\nblocks.4.1.bn1.act\nblocks.4.1.conv_dw\nblocks.4.1.bn2\nblocks.4.1.bn2.drop\nblocks.4.1.bn2.act\nblocks.4.1.aa\nblocks.4.1.se\nblocks.4.1.conv_pwl\nblocks.4.1.bn3\nblocks.4.1.bn3.drop\nblocks.4.1.bn3.act\nblocks.4.1.drop_path\nblocks.4.2\nblocks.4.2.conv_pw\nblocks.4.2.bn1\nblocks.4.2.bn1.drop\nblocks.4.2.bn1.act\nblocks.4.2.conv_dw\nblocks.4.2.bn2\nblocks.4.2.bn2.drop\nblocks.4.2.bn2.act\nblocks.4.2.aa\nblocks.4.2.se\nblocks.4.2.conv_pwl\nblocks.4.2.bn3\nblocks.4.2.bn3.drop\nblocks.4.2.bn3.act\nblocks.4.2.drop_path\nblocks.5\nblocks.5.0\nblocks.5.0.conv_pw\nblocks.5.0.bn1\nblocks.5.0.bn1.drop\nblocks.5.0.bn1.act\nblocks.5.0.conv_dw\nblocks.5.0.bn2\nblocks.5.0.bn2.drop\nblocks.5.0.bn2.act\nblocks.5.0.aa\nblocks.5.0.se\nblocks.5.0.conv_pwl\nblocks.5.0.bn3\nblocks.5.0.bn3.drop\nblocks.5.0.bn3.act\nblocks.5.0.drop_path\nblocks.5.1\nblocks.5.1.conv_pw\nblocks.5.1.bn1\nblocks.5.1.bn1.drop\nblocks.5.1.bn1.act\nblocks.5.1.conv_dw\nblocks.5.1.bn2\nblocks.5.1.bn2.drop\nblocks.5.1.bn2.act\nblocks.5.1.aa\nblocks.5.1.se\nblocks.5.1.conv_pwl\nblocks.5.1.bn3\nblocks.5.1.bn3.drop\nblocks.5.1.bn3.act\nblocks.5.1.drop_path\nblocks.5.2\nblocks.5.2.conv_pw\nblocks.5.2.bn1\nblocks.5.2.bn1.drop\nblocks.5.2.bn1.act\nblocks.5.2.conv_dw\nblocks.5.2.bn2\nblocks.5.2.bn2.drop\nblocks.5.2.bn2.act\nblocks.5.2.aa\nblocks.5.2.se\nblocks.5.2.conv_pwl\nblocks.5.2.bn3\nblocks.5.2.bn3.drop\nblocks.5.2.bn3.act\nblocks.5.2.drop_path\nblocks.6\nblocks.6.0\nblocks.6.0.conv_pw\nblocks.6.0.bn1\nblocks.6.0.bn1.drop\nblocks.6.0.bn1.act\nblocks.6.0.conv_dw\nblocks.6.0.bn2\nblocks.6.0.bn2.drop\nblocks.6.0.bn2.act\nblocks.6.0.aa\nblocks.6.0.se\nblocks.6.0.conv_pwl\nblocks.6.0.bn3\nblocks.6.0.bn3.drop\nblocks.6.0.bn3.act\nblocks.6.0.drop_path\nconv_head\nbn2\nbn2.drop\nbn2.act\nglobal_pool\nglobal_pool.pool\nglobal_pool.flatten\nclassifier\n\nEfficientNetB0 named modules:\n\nconv_stem\nbn1\nbn1.drop\nbn1.act\nblocks\nblocks.0\nblocks.0.0\nblocks.0.0.conv_dw\nblocks.0.0.bn1\nblocks.0.0.bn1.drop\nblocks.0.0.bn1.act\nblocks.0.0.aa\nblocks.0.0.se\nblocks.0.0.se.conv_reduce\nblocks.0.0.se.act1\nblocks.0.0.se.conv_expand\nblocks.0.0.se.gate\nblocks.0.0.conv_pw\nblocks.0.0.bn2\nblocks.0.0.bn2.drop\nblocks.0.0.bn2.act\nblocks.0.0.drop_path\nblocks.1\nblocks.1.0\nblocks.1.0.conv_pw\nblocks.1.0.bn1\nblocks.1.0.bn1.drop\nblocks.1.0.bn1.act\nblocks.1.0.conv_dw\nblocks.1.0.bn2\nblocks.1.0.bn2.drop\nblocks.1.0.bn2.act\nblocks.1.0.aa\nblocks.1.0.se\nblocks.1.0.se.conv_reduce\nblocks.1.0.se.act1\nblocks.1.0.se.conv_expand\nblocks.1.0.se.gate\nblocks.1.0.conv_pwl\nblocks.1.0.bn3\nblocks.1.0.bn3.drop\nblocks.1.0.bn3.act\nblocks.1.0.drop_path\nblocks.1.1\nblocks.1.1.conv_pw\nblocks.1.1.bn1\nblocks.1.1.bn1.drop\nblocks.1.1.bn1.act\nblocks.1.1.conv_dw\nblocks.1.1.bn2\nblocks.1.1.bn2.drop\nblocks.1.1.bn2.act\nblocks.1.1.aa\nblocks.1.1.se\nblocks.1.1.se.conv_reduce\nblocks.1.1.se.act1\nblocks.1.1.se.conv_expand\nblocks.1.1.se.gate\nblocks.1.1.conv_pwl\nblocks.1.1.bn3\nblocks.1.1.bn3.drop\nblocks.1.1.bn3.act\nblocks.1.1.drop_path\nblocks.2\nblocks.2.0\nblocks.2.0.conv_pw\nblocks.2.0.bn1\nblocks.2.0.bn1.drop\nblocks.2.0.bn1.act\nblocks.2.0.conv_dw\nblocks.2.0.bn2\nblocks.2.0.bn2.drop\nblocks.2.0.bn2.act\nblocks.2.0.aa\nblocks.2.0.se\nblocks.2.0.se.conv_reduce\nblocks.2.0.se.act1\nblocks.2.0.se.conv_expand\nblocks.2.0.se.gate\nblocks.2.0.conv_pwl\nblocks.2.0.bn3\nblocks.2.0.bn3.drop\nblocks.2.0.bn3.act\nblocks.2.0.drop_path\nblocks.2.1\nblocks.2.1.conv_pw\nblocks.2.1.bn1\nblocks.2.1.bn1.drop\nblocks.2.1.bn1.act\nblocks.2.1.conv_dw\nblocks.2.1.bn2\nblocks.2.1.bn2.drop\nblocks.2.1.bn2.act\nblocks.2.1.aa\nblocks.2.1.se\nblocks.2.1.se.conv_reduce\nblocks.2.1.se.act1\nblocks.2.1.se.conv_expand\nblocks.2.1.se.gate\nblocks.2.1.conv_pwl\nblocks.2.1.bn3\nblocks.2.1.bn3.drop\nblocks.2.1.bn3.act\nblocks.2.1.drop_path\nblocks.3\nblocks.3.0\nblocks.3.0.conv_pw\nblocks.3.0.bn1\nblocks.3.0.bn1.drop\nblocks.3.0.bn1.act\nblocks.3.0.conv_dw\nblocks.3.0.bn2\nblocks.3.0.bn2.drop\nblocks.3.0.bn2.act\nblocks.3.0.aa\nblocks.3.0.se\nblocks.3.0.se.conv_reduce\nblocks.3.0.se.act1\nblocks.3.0.se.conv_expand\nblocks.3.0.se.gate\nblocks.3.0.conv_pwl\nblocks.3.0.bn3\nblocks.3.0.bn3.drop\nblocks.3.0.bn3.act\nblocks.3.0.drop_path\nblocks.3.1\nblocks.3.1.conv_pw\nblocks.3.1.bn1\nblocks.3.1.bn1.drop\nblocks.3.1.bn1.act\nblocks.3.1.conv_dw\nblocks.3.1.bn2\nblocks.3.1.bn2.drop\nblocks.3.1.bn2.act\nblocks.3.1.aa\nblocks.3.1.se\nblocks.3.1.se.conv_reduce\nblocks.3.1.se.act1\nblocks.3.1.se.conv_expand\nblocks.3.1.se.gate\nblocks.3.1.conv_pwl\nblocks.3.1.bn3\nblocks.3.1.bn3.drop\nblocks.3.1.bn3.act\nblocks.3.1.drop_path\nblocks.3.2\nblocks.3.2.conv_pw\nblocks.3.2.bn1\nblocks.3.2.bn1.drop\nblocks.3.2.bn1.act\nblocks.3.2.conv_dw\nblocks.3.2.bn2\nblocks.3.2.bn2.drop\nblocks.3.2.bn2.act\nblocks.3.2.aa\nblocks.3.2.se\nblocks.3.2.se.conv_reduce\nblocks.3.2.se.act1\nblocks.3.2.se.conv_expand\nblocks.3.2.se.gate\nblocks.3.2.conv_pwl\nblocks.3.2.bn3\nblocks.3.2.bn3.drop\nblocks.3.2.bn3.act\nblocks.3.2.drop_path\nblocks.4\nblocks.4.0\nblocks.4.0.conv_pw\nblocks.4.0.bn1\nblocks.4.0.bn1.drop\nblocks.4.0.bn1.act\nblocks.4.0.conv_dw\nblocks.4.0.bn2\nblocks.4.0.bn2.drop\nblocks.4.0.bn2.act\nblocks.4.0.aa\nblocks.4.0.se\nblocks.4.0.se.conv_reduce\nblocks.4.0.se.act1\nblocks.4.0.se.conv_expand\nblocks.4.0.se.gate\nblocks.4.0.conv_pwl\nblocks.4.0.bn3\nblocks.4.0.bn3.drop\nblocks.4.0.bn3.act\nblocks.4.0.drop_path\nblocks.4.1\nblocks.4.1.conv_pw\nblocks.4.1.bn1\nblocks.4.1.bn1.drop\nblocks.4.1.bn1.act\nblocks.4.1.conv_dw\nblocks.4.1.bn2\nblocks.4.1.bn2.drop\nblocks.4.1.bn2.act\nblocks.4.1.aa\nblocks.4.1.se\nblocks.4.1.se.conv_reduce\nblocks.4.1.se.act1\nblocks.4.1.se.conv_expand\nblocks.4.1.se.gate\nblocks.4.1.conv_pwl\nblocks.4.1.bn3\nblocks.4.1.bn3.drop\nblocks.4.1.bn3.act\nblocks.4.1.drop_path\nblocks.4.2\nblocks.4.2.conv_pw\nblocks.4.2.bn1\nblocks.4.2.bn1.drop\nblocks.4.2.bn1.act\nblocks.4.2.conv_dw\nblocks.4.2.bn2\nblocks.4.2.bn2.drop\nblocks.4.2.bn2.act\nblocks.4.2.aa\nblocks.4.2.se\nblocks.4.2.se.conv_reduce\nblocks.4.2.se.act1\nblocks.4.2.se.conv_expand\nblocks.4.2.se.gate\nblocks.4.2.conv_pwl\nblocks.4.2.bn3\nblocks.4.2.bn3.drop\nblocks.4.2.bn3.act\nblocks.4.2.drop_path\nblocks.5\nblocks.5.0\nblocks.5.0.conv_pw\nblocks.5.0.bn1\nblocks.5.0.bn1.drop\nblocks.5.0.bn1.act\nblocks.5.0.conv_dw\nblocks.5.0.bn2\nblocks.5.0.bn2.drop\nblocks.5.0.bn2.act\nblocks.5.0.aa\nblocks.5.0.se\nblocks.5.0.se.conv_reduce\nblocks.5.0.se.act1\nblocks.5.0.se.conv_expand\nblocks.5.0.se.gate\nblocks.5.0.conv_pwl\nblocks.5.0.bn3\nblocks.5.0.bn3.drop\nblocks.5.0.bn3.act\nblocks.5.0.drop_path\nblocks.5.1\nblocks.5.1.conv_pw\nblocks.5.1.bn1\nblocks.5.1.bn1.drop\nblocks.5.1.bn1.act\nblocks.5.1.conv_dw\nblocks.5.1.bn2\nblocks.5.1.bn2.drop\nblocks.5.1.bn2.act\nblocks.5.1.aa\nblocks.5.1.se\nblocks.5.1.se.conv_reduce\nblocks.5.1.se.act1\nblocks.5.1.se.conv_expand\nblocks.5.1.se.gate\nblocks.5.1.conv_pwl\nblocks.5.1.bn3\nblocks.5.1.bn3.drop\nblocks.5.1.bn3.act\nblocks.5.1.drop_path\nblocks.5.2\nblocks.5.2.conv_pw\nblocks.5.2.bn1\nblocks.5.2.bn1.drop\nblocks.5.2.bn1.act\nblocks.5.2.conv_dw\nblocks.5.2.bn2\nblocks.5.2.bn2.drop\nblocks.5.2.bn2.act\nblocks.5.2.aa\nblocks.5.2.se\nblocks.5.2.se.conv_reduce\nblocks.5.2.se.act1\nblocks.5.2.se.conv_expand\nblocks.5.2.se.gate\nblocks.5.2.conv_pwl\nblocks.5.2.bn3\nblocks.5.2.bn3.drop\nblocks.5.2.bn3.act\nblocks.5.2.drop_path\nblocks.5.3\nblocks.5.3.conv_pw\nblocks.5.3.bn1\nblocks.5.3.bn1.drop\nblocks.5.3.bn1.act\nblocks.5.3.conv_dw\nblocks.5.3.bn2\nblocks.5.3.bn2.drop\nblocks.5.3.bn2.act\nblocks.5.3.aa\nblocks.5.3.se\nblocks.5.3.se.conv_reduce\nblocks.5.3.se.act1\nblocks.5.3.se.conv_expand\nblocks.5.3.se.gate\nblocks.5.3.conv_pwl\nblocks.5.3.bn3\nblocks.5.3.bn3.drop\nblocks.5.3.bn3.act\nblocks.5.3.drop_path\nblocks.6\nblocks.6.0\nblocks.6.0.conv_pw\nblocks.6.0.bn1\nblocks.6.0.bn1.drop\nblocks.6.0.bn1.act\nblocks.6.0.conv_dw\nblocks.6.0.bn2\nblocks.6.0.bn2.drop\nblocks.6.0.bn2.act\nblocks.6.0.aa\nblocks.6.0.se\nblocks.6.0.se.conv_reduce\nblocks.6.0.se.act1\nblocks.6.0.se.conv_expand\nblocks.6.0.se.gate\nblocks.6.0.conv_pwl\nblocks.6.0.bn3\nblocks.6.0.bn3.drop\nblocks.6.0.bn3.act\nblocks.6.0.drop_path\nconv_head\nbn2\nbn2.drop\nbn2.act\nglobal_pool\nglobal_pool.pool\nglobal_pool.flatten\nclassifier\n\nViT Base Patch16_224 named modules:\n\npatch_embed\npatch_embed.proj\npatch_embed.norm\npos_drop\npatch_drop\nnorm_pre\nblocks\nblocks.0\nblocks.0.norm1\nblocks.0.attn\nblocks.0.attn.qkv\nblocks.0.attn.q_norm\nblocks.0.attn.k_norm\nblocks.0.attn.attn_drop\nblocks.0.attn.norm\nblocks.0.attn.proj\nblocks.0.attn.proj_drop\nblocks.0.ls1\nblocks.0.drop_path1\nblocks.0.norm2\nblocks.0.mlp\nblocks.0.mlp.fc1\nblocks.0.mlp.act\nblocks.0.mlp.drop1\nblocks.0.mlp.norm\nblocks.0.mlp.fc2\nblocks.0.mlp.drop2\nblocks.0.ls2\nblocks.0.drop_path2\nblocks.1\nblocks.1.norm1\nblocks.1.attn\nblocks.1.attn.qkv\nblocks.1.attn.q_norm\nblocks.1.attn.k_norm\nblocks.1.attn.attn_drop\nblocks.1.attn.norm\nblocks.1.attn.proj\nblocks.1.attn.proj_drop\nblocks.1.ls1\nblocks.1.drop_path1\nblocks.1.norm2\nblocks.1.mlp\nblocks.1.mlp.fc1\nblocks.1.mlp.act\nblocks.1.mlp.drop1\nblocks.1.mlp.norm\nblocks.1.mlp.fc2\nblocks.1.mlp.drop2\nblocks.1.ls2\nblocks.1.drop_path2\nblocks.2\nblocks.2.norm1\nblocks.2.attn\nblocks.2.attn.qkv\nblocks.2.attn.q_norm\nblocks.2.attn.k_norm\nblocks.2.attn.attn_drop\nblocks.2.attn.norm\nblocks.2.attn.proj\nblocks.2.attn.proj_drop\nblocks.2.ls1\nblocks.2.drop_path1\nblocks.2.norm2\nblocks.2.mlp\nblocks.2.mlp.fc1\nblocks.2.mlp.act\nblocks.2.mlp.drop1\nblocks.2.mlp.norm\nblocks.2.mlp.fc2\nblocks.2.mlp.drop2\nblocks.2.ls2\nblocks.2.drop_path2\nblocks.3\nblocks.3.norm1\nblocks.3.attn\nblocks.3.attn.qkv\nblocks.3.attn.q_norm\nblocks.3.attn.k_norm\nblocks.3.attn.attn_drop\nblocks.3.attn.norm\nblocks.3.attn.proj\nblocks.3.attn.proj_drop\nblocks.3.ls1\nblocks.3.drop_path1\nblocks.3.norm2\nblocks.3.mlp\nblocks.3.mlp.fc1\nblocks.3.mlp.act\nblocks.3.mlp.drop1\nblocks.3.mlp.norm\nblocks.3.mlp.fc2\nblocks.3.mlp.drop2\nblocks.3.ls2\nblocks.3.drop_path2\nblocks.4\nblocks.4.norm1\nblocks.4.attn\nblocks.4.attn.qkv\nblocks.4.attn.q_norm\nblocks.4.attn.k_norm\nblocks.4.attn.attn_drop\nblocks.4.attn.norm\nblocks.4.attn.proj\nblocks.4.attn.proj_drop\nblocks.4.ls1\nblocks.4.drop_path1\nblocks.4.norm2\nblocks.4.mlp\nblocks.4.mlp.fc1\nblocks.4.mlp.act\nblocks.4.mlp.drop1\nblocks.4.mlp.norm\nblocks.4.mlp.fc2\nblocks.4.mlp.drop2\nblocks.4.ls2\nblocks.4.drop_path2\nblocks.5\nblocks.5.norm1\nblocks.5.attn\nblocks.5.attn.qkv\nblocks.5.attn.q_norm\nblocks.5.attn.k_norm\nblocks.5.attn.attn_drop\nblocks.5.attn.norm\nblocks.5.attn.proj\nblocks.5.attn.proj_drop\nblocks.5.ls1\nblocks.5.drop_path1\nblocks.5.norm2\nblocks.5.mlp\nblocks.5.mlp.fc1\nblocks.5.mlp.act\nblocks.5.mlp.drop1\nblocks.5.mlp.norm\nblocks.5.mlp.fc2\nblocks.5.mlp.drop2\nblocks.5.ls2\nblocks.5.drop_path2\nblocks.6\nblocks.6.norm1\nblocks.6.attn\nblocks.6.attn.qkv\nblocks.6.attn.q_norm\nblocks.6.attn.k_norm\nblocks.6.attn.attn_drop\nblocks.6.attn.norm\nblocks.6.attn.proj\nblocks.6.attn.proj_drop\nblocks.6.ls1\nblocks.6.drop_path1\nblocks.6.norm2\nblocks.6.mlp\nblocks.6.mlp.fc1\nblocks.6.mlp.act\nblocks.6.mlp.drop1\nblocks.6.mlp.norm\nblocks.6.mlp.fc2\nblocks.6.mlp.drop2\nblocks.6.ls2\nblocks.6.drop_path2\nblocks.7\nblocks.7.norm1\nblocks.7.attn\nblocks.7.attn.qkv\nblocks.7.attn.q_norm\nblocks.7.attn.k_norm\nblocks.7.attn.attn_drop\nblocks.7.attn.norm\nblocks.7.attn.proj\nblocks.7.attn.proj_drop\nblocks.7.ls1\nblocks.7.drop_path1\nblocks.7.norm2\nblocks.7.mlp\nblocks.7.mlp.fc1\nblocks.7.mlp.act\nblocks.7.mlp.drop1\nblocks.7.mlp.norm\nblocks.7.mlp.fc2\nblocks.7.mlp.drop2\nblocks.7.ls2\nblocks.7.drop_path2\nblocks.8\nblocks.8.norm1\nblocks.8.attn\nblocks.8.attn.qkv\nblocks.8.attn.q_norm\nblocks.8.attn.k_norm\nblocks.8.attn.attn_drop\nblocks.8.attn.norm\nblocks.8.attn.proj\nblocks.8.attn.proj_drop\nblocks.8.ls1\nblocks.8.drop_path1\nblocks.8.norm2\nblocks.8.mlp\nblocks.8.mlp.fc1\nblocks.8.mlp.act\nblocks.8.mlp.drop1\nblocks.8.mlp.norm\nblocks.8.mlp.fc2\nblocks.8.mlp.drop2\nblocks.8.ls2\nblocks.8.drop_path2\nblocks.9\nblocks.9.norm1\nblocks.9.attn\nblocks.9.attn.qkv\nblocks.9.attn.q_norm\nblocks.9.attn.k_norm\nblocks.9.attn.attn_drop\nblocks.9.attn.norm\nblocks.9.attn.proj\nblocks.9.attn.proj_drop\nblocks.9.ls1\nblocks.9.drop_path1\nblocks.9.norm2\nblocks.9.mlp\nblocks.9.mlp.fc1\nblocks.9.mlp.act\nblocks.9.mlp.drop1\nblocks.9.mlp.norm\nblocks.9.mlp.fc2\nblocks.9.mlp.drop2\nblocks.9.ls2\nblocks.9.drop_path2\nblocks.10\nblocks.10.norm1\nblocks.10.attn\nblocks.10.attn.qkv\nblocks.10.attn.q_norm\nblocks.10.attn.k_norm\nblocks.10.attn.attn_drop\nblocks.10.attn.norm\nblocks.10.attn.proj\nblocks.10.attn.proj_drop\nblocks.10.ls1\nblocks.10.drop_path1\nblocks.10.norm2\nblocks.10.mlp\nblocks.10.mlp.fc1\nblocks.10.mlp.act\nblocks.10.mlp.drop1\nblocks.10.mlp.norm\nblocks.10.mlp.fc2\nblocks.10.mlp.drop2\nblocks.10.ls2\nblocks.10.drop_path2\nblocks.11\nblocks.11.norm1\nblocks.11.attn\nblocks.11.attn.qkv\nblocks.11.attn.q_norm\nblocks.11.attn.k_norm\nblocks.11.attn.attn_drop\nblocks.11.attn.norm\nblocks.11.attn.proj\nblocks.11.attn.proj_drop\nblocks.11.ls1\nblocks.11.drop_path1\nblocks.11.norm2\nblocks.11.mlp\nblocks.11.mlp.fc1\nblocks.11.mlp.act\nblocks.11.mlp.drop1\nblocks.11.mlp.norm\nblocks.11.mlp.fc2\nblocks.11.mlp.drop2\nblocks.11.ls2\nblocks.11.drop_path2\nnorm\nfc_norm\nhead_drop\nhead\n\nSwin Tiny Patch4 Window7_224 named modules:\n\npatch_embed\npatch_embed.proj\npatch_embed.norm\nlayers\nlayers.0\nlayers.0.downsample\nlayers.0.blocks\nlayers.0.blocks.0\nlayers.0.blocks.0.norm1\nlayers.0.blocks.0.attn\nlayers.0.blocks.0.attn.qkv\nlayers.0.blocks.0.attn.attn_drop\nlayers.0.blocks.0.attn.proj\nlayers.0.blocks.0.attn.proj_drop\nlayers.0.blocks.0.attn.softmax\nlayers.0.blocks.0.drop_path1\nlayers.0.blocks.0.norm2\nlayers.0.blocks.0.mlp\nlayers.0.blocks.0.mlp.fc1\nlayers.0.blocks.0.mlp.act\nlayers.0.blocks.0.mlp.drop1\nlayers.0.blocks.0.mlp.norm\nlayers.0.blocks.0.mlp.fc2\nlayers.0.blocks.0.mlp.drop2\nlayers.0.blocks.0.drop_path2\nlayers.0.blocks.1\nlayers.0.blocks.1.norm1\nlayers.0.blocks.1.attn\nlayers.0.blocks.1.attn.qkv\nlayers.0.blocks.1.attn.attn_drop\nlayers.0.blocks.1.attn.proj\nlayers.0.blocks.1.attn.proj_drop\nlayers.0.blocks.1.attn.softmax\nlayers.0.blocks.1.drop_path1\nlayers.0.blocks.1.norm2\nlayers.0.blocks.1.mlp\nlayers.0.blocks.1.mlp.fc1\nlayers.0.blocks.1.mlp.act\nlayers.0.blocks.1.mlp.drop1\nlayers.0.blocks.1.mlp.norm\nlayers.0.blocks.1.mlp.fc2\nlayers.0.blocks.1.mlp.drop2\nlayers.0.blocks.1.drop_path2\nlayers.1\nlayers.1.downsample\nlayers.1.downsample.norm\nlayers.1.downsample.reduction\nlayers.1.blocks\nlayers.1.blocks.0\nlayers.1.blocks.0.norm1\nlayers.1.blocks.0.attn\nlayers.1.blocks.0.attn.qkv\nlayers.1.blocks.0.attn.attn_drop\nlayers.1.blocks.0.attn.proj\nlayers.1.blocks.0.attn.proj_drop\nlayers.1.blocks.0.attn.softmax\nlayers.1.blocks.0.drop_path1\nlayers.1.blocks.0.norm2\nlayers.1.blocks.0.mlp\nlayers.1.blocks.0.mlp.fc1\nlayers.1.blocks.0.mlp.act\nlayers.1.blocks.0.mlp.drop1\nlayers.1.blocks.0.mlp.norm\nlayers.1.blocks.0.mlp.fc2\nlayers.1.blocks.0.mlp.drop2\nlayers.1.blocks.0.drop_path2\nlayers.1.blocks.1\nlayers.1.blocks.1.norm1\nlayers.1.blocks.1.attn\nlayers.1.blocks.1.attn.qkv\nlayers.1.blocks.1.attn.attn_drop\nlayers.1.blocks.1.attn.proj\nlayers.1.blocks.1.attn.proj_drop\nlayers.1.blocks.1.attn.softmax\nlayers.1.blocks.1.drop_path1\nlayers.1.blocks.1.norm2\nlayers.1.blocks.1.mlp\nlayers.1.blocks.1.mlp.fc1\nlayers.1.blocks.1.mlp.act\nlayers.1.blocks.1.mlp.drop1\nlayers.1.blocks.1.mlp.norm\nlayers.1.blocks.1.mlp.fc2\nlayers.1.blocks.1.mlp.drop2\nlayers.1.blocks.1.drop_path2\nlayers.2\nlayers.2.downsample\nlayers.2.downsample.norm\nlayers.2.downsample.reduction\nlayers.2.blocks\nlayers.2.blocks.0\nlayers.2.blocks.0.norm1\nlayers.2.blocks.0.attn\nlayers.2.blocks.0.attn.qkv\nlayers.2.blocks.0.attn.attn_drop\nlayers.2.blocks.0.attn.proj\nlayers.2.blocks.0.attn.proj_drop\nlayers.2.blocks.0.attn.softmax\nlayers.2.blocks.0.drop_path1\nlayers.2.blocks.0.norm2\nlayers.2.blocks.0.mlp\nlayers.2.blocks.0.mlp.fc1\nlayers.2.blocks.0.mlp.act\nlayers.2.blocks.0.mlp.drop1\nlayers.2.blocks.0.mlp.norm\nlayers.2.blocks.0.mlp.fc2\nlayers.2.blocks.0.mlp.drop2\nlayers.2.blocks.0.drop_path2\nlayers.2.blocks.1\nlayers.2.blocks.1.norm1\nlayers.2.blocks.1.attn\nlayers.2.blocks.1.attn.qkv\nlayers.2.blocks.1.attn.attn_drop\nlayers.2.blocks.1.attn.proj\nlayers.2.blocks.1.attn.proj_drop\nlayers.2.blocks.1.attn.softmax\nlayers.2.blocks.1.drop_path1\nlayers.2.blocks.1.norm2\nlayers.2.blocks.1.mlp\nlayers.2.blocks.1.mlp.fc1\nlayers.2.blocks.1.mlp.act\nlayers.2.blocks.1.mlp.drop1\nlayers.2.blocks.1.mlp.norm\nlayers.2.blocks.1.mlp.fc2\nlayers.2.blocks.1.mlp.drop2\nlayers.2.blocks.1.drop_path2\nlayers.2.blocks.2\nlayers.2.blocks.2.norm1\nlayers.2.blocks.2.attn\nlayers.2.blocks.2.attn.qkv\nlayers.2.blocks.2.attn.attn_drop\nlayers.2.blocks.2.attn.proj\nlayers.2.blocks.2.attn.proj_drop\nlayers.2.blocks.2.attn.softmax\nlayers.2.blocks.2.drop_path1\nlayers.2.blocks.2.norm2\nlayers.2.blocks.2.mlp\nlayers.2.blocks.2.mlp.fc1\nlayers.2.blocks.2.mlp.act\nlayers.2.blocks.2.mlp.drop1\nlayers.2.blocks.2.mlp.norm\nlayers.2.blocks.2.mlp.fc2\nlayers.2.blocks.2.mlp.drop2\nlayers.2.blocks.2.drop_path2\nlayers.2.blocks.3\nlayers.2.blocks.3.norm1\nlayers.2.blocks.3.attn\nlayers.2.blocks.3.attn.qkv\nlayers.2.blocks.3.attn.attn_drop\nlayers.2.blocks.3.attn.proj\nlayers.2.blocks.3.attn.proj_drop\nlayers.2.blocks.3.attn.softmax\nlayers.2.blocks.3.drop_path1\nlayers.2.blocks.3.norm2\nlayers.2.blocks.3.mlp\nlayers.2.blocks.3.mlp.fc1\nlayers.2.blocks.3.mlp.act\nlayers.2.blocks.3.mlp.drop1\nlayers.2.blocks.3.mlp.norm\nlayers.2.blocks.3.mlp.fc2\nlayers.2.blocks.3.mlp.drop2\nlayers.2.blocks.3.drop_path2\nlayers.2.blocks.4\nlayers.2.blocks.4.norm1\nlayers.2.blocks.4.attn\nlayers.2.blocks.4.attn.qkv\nlayers.2.blocks.4.attn.attn_drop\nlayers.2.blocks.4.attn.proj\nlayers.2.blocks.4.attn.proj_drop\nlayers.2.blocks.4.attn.softmax\nlayers.2.blocks.4.drop_path1\nlayers.2.blocks.4.norm2\nlayers.2.blocks.4.mlp\nlayers.2.blocks.4.mlp.fc1\nlayers.2.blocks.4.mlp.act\nlayers.2.blocks.4.mlp.drop1\nlayers.2.blocks.4.mlp.norm\nlayers.2.blocks.4.mlp.fc2\nlayers.2.blocks.4.mlp.drop2\nlayers.2.blocks.4.drop_path2\nlayers.2.blocks.5\nlayers.2.blocks.5.norm1\nlayers.2.blocks.5.attn\nlayers.2.blocks.5.attn.qkv\nlayers.2.blocks.5.attn.attn_drop\nlayers.2.blocks.5.attn.proj\nlayers.2.blocks.5.attn.proj_drop\nlayers.2.blocks.5.attn.softmax\nlayers.2.blocks.5.drop_path1\nlayers.2.blocks.5.norm2\nlayers.2.blocks.5.mlp\nlayers.2.blocks.5.mlp.fc1\nlayers.2.blocks.5.mlp.act\nlayers.2.blocks.5.mlp.drop1\nlayers.2.blocks.5.mlp.norm\nlayers.2.blocks.5.mlp.fc2\nlayers.2.blocks.5.mlp.drop2\nlayers.2.blocks.5.drop_path2\nlayers.3\nlayers.3.downsample\nlayers.3.downsample.norm\nlayers.3.downsample.reduction\nlayers.3.blocks\nlayers.3.blocks.0\nlayers.3.blocks.0.norm1\nlayers.3.blocks.0.attn\nlayers.3.blocks.0.attn.qkv\nlayers.3.blocks.0.attn.attn_drop\nlayers.3.blocks.0.attn.proj\nlayers.3.blocks.0.attn.proj_drop\nlayers.3.blocks.0.attn.softmax\nlayers.3.blocks.0.drop_path1\nlayers.3.blocks.0.norm2\nlayers.3.blocks.0.mlp\nlayers.3.blocks.0.mlp.fc1\nlayers.3.blocks.0.mlp.act\nlayers.3.blocks.0.mlp.drop1\nlayers.3.blocks.0.mlp.norm\nlayers.3.blocks.0.mlp.fc2\nlayers.3.blocks.0.mlp.drop2\nlayers.3.blocks.0.drop_path2\nlayers.3.blocks.1\nlayers.3.blocks.1.norm1\nlayers.3.blocks.1.attn\nlayers.3.blocks.1.attn.qkv\nlayers.3.blocks.1.attn.attn_drop\nlayers.3.blocks.1.attn.proj\nlayers.3.blocks.1.attn.proj_drop\nlayers.3.blocks.1.attn.softmax\nlayers.3.blocks.1.drop_path1\nlayers.3.blocks.1.norm2\nlayers.3.blocks.1.mlp\nlayers.3.blocks.1.mlp.fc1\nlayers.3.blocks.1.mlp.act\nlayers.3.blocks.1.mlp.drop1\nlayers.3.blocks.1.mlp.norm\nlayers.3.blocks.1.mlp.fc2\nlayers.3.blocks.1.mlp.drop2\nlayers.3.blocks.1.drop_path2\nnorm\nhead\nhead.global_pool\nhead.global_pool.pool\nhead.global_pool.flatten\nhead.drop\nhead.fc\nhead.flatten\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport timm\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"NOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS CORRECT TARGET LAYERS\")\nprint(\"=\"*80)\n\n# ========== CONFIG ==========\nTEST_DIR = '/kaggle/input/split-dataset/test'\nWEIGHTS_DIR = '/kaggle/input/weight/weights'\nOUTPUT_DIR = '/kaggle/working'\nIMG_SIZE = 224\nNUM_IMAGES_PER_CLASS = 200\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n✓ Device: {DEVICE}\")\nprint(f\"✓ Test dataset: {TEST_DIR}\")\nprint(f\"✓ Weights directory: {WEIGHTS_DIR}\")\nprint(f\"✓ Output directory: {OUTPUT_DIR}\")\n\n# ========== 1. LOAD TEST IMAGES ==========\nprint(\"\\n[1/5] Loading test images...\")\n\nclass_names = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\ntest_images = {cls: [] for cls in class_names}\n\nfor cls in class_names:\n    cls_path = os.path.join(TEST_DIR, cls)\n    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    sampled = random.sample(images, min(NUM_IMAGES_PER_CLASS, len(images)))\n    test_images[cls] = [os.path.join(cls_path, img) for img in sampled]\n\nfor cls in class_names:\n    print(f\"  {cls}: {len(test_images[cls])}\")\ntotal_images = sum(len(v) for v in test_images.values())\nprint(f\"✓ Total test images: {total_images}\")\n\n# ========== 2. PREPROCESS ==========\nprint(\"\\n[2/5] Setting up preprocessing...\")\n\ndef preprocess_image(img_path):\n    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    image_norm = image_rgb.astype(np.float32) / 255.0\n    image_norm = (image_norm - 0.5) / 0.5\n    img_tensor = torch.from_numpy(image_norm).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n    img_tensor.requires_grad_(True)  # Enable grad for CAM backward\n    return img_tensor, image_rgb\n\nprint(\"✓ Preprocessing ready\")\n\n# ========== 3. CUSTOM CLASSIFIER HEAD (for CNN models) ==========\nclass CustomHead(nn.Module):\n    def __init__(self, in_features, num_classes=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\n# ========== 4. LOAD MODELS ==========\nprint(\"\\n[3/5] Loading trained models...\")\n\nmodel_configs = {\n    'resnet50': {\n        'model_fn': lambda: timm.create_model('resnet50', pretrained=False, num_classes=4),\n        'weight_file': 'resnet50_best_80_10_10.pth',\n        'target_layer': 'layer4.2',\n        'has_custom_head': True,\n        'num_features': 2048\n    },\n    'mobilenetv2': {\n        'model_fn': lambda: timm.create_model('mobilenetv2_100', pretrained=False, num_classes=4),\n        'weight_file': 'mobilenetv2_best_80_10_10.pth',\n        'target_layer': 'blocks.18',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'efficientnetb0': {\n        'model_fn': lambda: timm.create_model('efficientnet_b0', pretrained=False, num_classes=4),\n        'weight_file': 'efficientnetb0_best_80_10_10.pth',\n        'target_layer': 'blocks.16',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'vit': {\n        'model_fn': lambda: timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4),\n        'weight_file': 'vit_best_80_10_10.pth',\n        'target_layer': 'blocks.11',\n        'has_custom_head': False\n    },\n    'swin': {\n        'model_fn': lambda: timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4),\n        'weight_file': 'swin_best_80_10_10.pth',\n        'target_layer': 'layers.2',\n        'has_custom_head': False\n    }\n}\n\nmodels = {}\nfor model_name, cfg in model_configs.items():\n    try:\n        model = cfg['model_fn']()\n        \n        if cfg['has_custom_head']:\n            model.fc = CustomHead(cfg['num_features'], num_classes=4)\n        \n        weight_path = os.path.join(WEIGHTS_DIR, cfg['weight_file'])\n        if os.path.exists(weight_path):\n            model.load_state_dict(torch.load(weight_path, map_location=DEVICE), strict=False)\n            model.to(DEVICE)\n            model.eval()\n            models[model_name] = model\n            print(f\"✓ {model_name.upper()} loaded from {weight_path}\")\n        else:\n            print(f\"⚠ {model_name.upper()} weights not found at {weight_path}\")\n    except Exception as e:\n        print(f\"✗ Error loading {model_name}: {e}\")\n\nprint(f\"\\n✓ Total models loaded: {len(models)}\")\n\n# ========== 5. GRAD-CAM ==========\nprint(\"\\n[4/5] Implementing Grad-CAM...\")\n\nclass GradCAM:\n    def __init__(self, model, target_layer_name):\n        self.model = model\n        self.target_layer_name = target_layer_name\n        self.gradients = None\n        self.activations = None\n        self.hooks = []\n        self._register_hooks()\n    \n    def _register_hooks(self):\n        def forward_hook(module, inp, out):\n            self.activations = out\n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0]\n        for name, module in self.model.named_modules():\n            if self.target_layer_name in name or name.endswith(self.target_layer_name):\n                self.hooks.append(module.register_forward_hook(forward_hook))\n                self.hooks.append(module.register_full_backward_hook(backward_hook))\n                break\n    \n    def generate_cam(self, input_tensor, class_idx=None):\n        output = self.model(input_tensor)\n        if class_idx is None:\n            class_idx = output.argmax(dim=1).item()\n        self.model.zero_grad()\n        score = output[0, class_idx]\n        score.backward(retain_graph=True)\n\n        grads = self.gradients.detach().cpu().numpy()[0]      # (C,H,W)\n        acts = self.activations.detach().cpu().numpy()[0]     # (C,H,W)\n        weights = grads.mean(axis=(1, 2))                     # (C,)\n        cam = np.zeros(acts.shape[1:], dtype=np.float32)\n        for c, w in enumerate(weights):\n            cam += w * acts[c]\n        cam = np.maximum(cam, 0)\n        cam = cam / (cam.max() + 1e-8)\n        return cam, class_idx\n    \n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n\nprint(\"✓ Grad-CAM ready\")\n\n# ========== 6. GENERATE CAMs ==========\nprint(\"\\n[5/5] Generating CAMs...\")\n\ncam_output_dir = os.path.join(OUTPUT_DIR, 'cam_visualizations')\nos.makedirs(cam_output_dir, exist_ok=True)\n\nall_cams = {}\n\nif len(models) == 0:\n    print(\"✗ No models loaded. Cannot generate CAMs.\")\nelse:\n    for model_name, model in tqdm(models.items(), desc=\"Processing models\"):\n        target_layer = model_configs[model_name]['target_layer']\n        cam_engine = GradCAM(model, target_layer)\n        all_cams[model_name] = {}\n        \n        for cls_name in class_names:\n            all_cams[model_name][cls_name] = []\n            for img_idx, img_path in enumerate(test_images[cls_name]):\n                try:\n                    img_tensor, img_rgb = preprocess_image(img_path)\n                    cam, pred_class = cam_engine.generate_cam(img_tensor)\n                    cam_resized = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n                    cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                    cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                    img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                    overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                    save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                    cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                    all_cams[model_name][cls_name].append({\n                        'path': img_path,\n                        'overlay': overlay,\n                        'pred_class': pred_class\n                    })\n                except Exception as e:\n                    pass  # Silently skip errors\n        \n        cam_engine.remove_hooks()\n        print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n\n    print(f\"\\n✓ All CAM overlays saved to: {cam_output_dir}\")\n\n    # ========== 7. CREATE COMPARISON GRIDS ==========\n    print(\"\\n[6/6] Creating comparison grids...\")\n\n    for cls_name in class_names:\n        fig, axes = plt.subplots(1, len(models) + 1, figsize=(24, 4))\n        first_img_path = test_images[cls_name][0]\n        _, img_rgb = preprocess_image(first_img_path)\n        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n        axes[0].imshow(img_rgb_uint8, cmap='gray')\n        axes[0].set_title(f'{cls_name} - Original', fontsize=11, fontweight='bold')\n        axes[0].axis('off')\n        for i, (model_name, _) in enumerate(models.items()):\n            if len(all_cams[model_name][cls_name]) > 0:\n                overlay = all_cams[model_name][cls_name][0]['overlay']\n                axes[i+1].imshow(overlay)\n                axes[i+1].set_title(model_name.upper(), fontsize=11, fontweight='bold')\n            axes[i+1].axis('off')\n        plt.tight_layout()\n        plt.savefig(os.path.join(cam_output_dir, f'comparison_grid_{cls_name}.png'), dpi=300)\n        plt.close()\n        print(f\"✓ Saved comparison grid for {cls_name}\")\n\n# ========== 8. SUMMARY ==========\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: GRAD-CAM ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n\nif len(models) > 0:\n    print(f\"\\nModels processed: {list(models.keys())}\")\n    print(f\"Classes analyzed: {class_names}\")\n    print(f\"Images per class: {NUM_IMAGES_PER_CLASS}\")\n    total_cams = sum(len(all_cams[m][c]) for m in models.keys() for c in class_names)\n    print(f\"Total CAM visualizations: {total_cams}\")\n    print(f\"\\nOutput: {cam_output_dir}\")\n    print(f\"Files: individual cam_<model>_lass>_<idx>.png + comparison_grid_lass>.png\")\n    print(\"\\n✓ All CAM visualizations saved successfully!\")\nelse:\n    print(\"\\n✗ No models loaded. Check weights.\")\n\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T21:58:07.945927Z","iopub.execute_input":"2025-11-26T21:58:07.946234Z","iopub.status.idle":"2025-11-26T22:00:24.766705Z","shell.execute_reply.started":"2025-11-26T21:58:07.946214Z","shell.execute_reply":"2025-11-26T22:00:24.766074Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nNOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS CORRECT TARGET LAYERS\n================================================================================\n\n✓ Device: cuda\n✓ Test dataset: /kaggle/input/split-dataset/test\n✓ Weights directory: /kaggle/input/weight/weights\n✓ Output directory: /kaggle/working\n\n[1/5] Loading test images...\n  CNV: 200\n  DME: 200\n  DRUSEN: 200\n  NORMAL: 200\n✓ Total test images: 800\n\n[2/5] Setting up preprocessing...\n✓ Preprocessing ready\n\n[3/5] Loading trained models...\n✓ RESNET50 loaded from /kaggle/input/weight/weights/resnet50_best_80_10_10.pth\n✓ MOBILENETV2 loaded from /kaggle/input/weight/weights/mobilenetv2_best_80_10_10.pth\n✓ EFFICIENTNETB0 loaded from /kaggle/input/weight/weights/efficientnetb0_best_80_10_10.pth\n✓ VIT loaded from /kaggle/input/weight/weights/vit_best_80_10_10.pth\n✓ SWIN loaded from /kaggle/input/weight/weights/swin_best_80_10_10.pth\n\n✓ Total models loaded: 5\n\n[4/5] Implementing Grad-CAM...\n✓ Grad-CAM ready\n\n[5/5] Generating CAMs...\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  20%|██        | 1/5 [00:26<01:45, 26.42s/it]","output_type":"stream"},{"name":"stdout","text":"✓ RESNET50: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  40%|████      | 2/5 [00:41<00:59, 19.95s/it]","output_type":"stream"},{"name":"stdout","text":"✓ MOBILENETV2: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  60%|██████    | 3/5 [01:03<00:41, 20.87s/it]","output_type":"stream"},{"name":"stdout","text":"✓ EFFICIENTNETB0: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  80%|████████  | 4/5 [01:37<00:25, 25.83s/it]","output_type":"stream"},{"name":"stdout","text":"✓ VIT: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models: 100%|██████████| 5/5 [02:10<00:00, 26.15s/it]","output_type":"stream"},{"name":"stdout","text":"✓ SWIN: 200 CAMs per class\n\n✓ All CAM overlays saved to: /kaggle/working/cam_visualizations\n\n[6/6] Creating comparison grids...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved comparison grid for CNV\n✓ Saved comparison grid for DME\n✓ Saved comparison grid for DRUSEN\n✓ Saved comparison grid for NORMAL\n\n================================================================================\nSUMMARY: GRAD-CAM ANALYSIS COMPLETE\n================================================================================\n\nModels processed: ['resnet50', 'mobilenetv2', 'efficientnetb0', 'vit', 'swin']\nClasses analyzed: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\nImages per class: 200\nTotal CAM visualizations: 1600\n\nOutput: /kaggle/working/cam_visualizations\nFiles: individual cam_<model>_lass>_<idx>.png + comparison_grid_lass>.png\n\n✓ All CAM visualizations saved successfully!\n================================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport timm\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"NOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS INCLUDING ViT ATTENTION CAM\")\nprint(\"=\"*80)\n\n# ========== CONFIG ==========\nTEST_DIR = '/kaggle/input/split-dataset/test'\nWEIGHTS_DIR = '/kaggle/input/weight/weights'\nOUTPUT_DIR = '/kaggle/working'\nIMG_SIZE = 224\nNUM_IMAGES_PER_CLASS = 200\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n✓ Device: {DEVICE}\")\nprint(f\"✓ Test dataset: {TEST_DIR}\")\nprint(f\"✓ Weights directory: {WEIGHTS_DIR}\")\nprint(f\"✓ Output directory: {OUTPUT_DIR}\")\n\n# ========== 1. LOAD TEST IMAGES ==========\nprint(\"\\n[1/5] Loading test images...\")\n\nclass_names = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\ntest_images = {cls: [] for cls in class_names}\n\nfor cls in class_names:\n    cls_path = os.path.join(TEST_DIR, cls)\n    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    sampled = random.sample(images, min(NUM_IMAGES_PER_CLASS, len(images)))\n    test_images[cls] = [os.path.join(cls_path, img) for img in sampled]\n\nfor cls in class_names:\n    print(f\"  {cls}: {len(test_images[cls])}\")\ntotal_images = sum(len(v) for v in test_images.values())\nprint(f\"✓ Total test images: {total_images}\")\n\n# ========== 2. PREPROCESS ==========\nprint(\"\\n[2/5] Setting up preprocessing...\")\n\ndef preprocess_image(img_path):\n    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    image_norm = image_rgb.astype(np.float32) / 255.0\n    image_norm = (image_norm - 0.5) / 0.5\n    img_tensor = torch.from_numpy(image_norm).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n    img_tensor.requires_grad_(True)  # Enable gradients for CAM\n    return img_tensor, image_rgb\n\nprint(\"✓ Preprocessing ready\")\n\n# ========== 3. CUSTOM CNN CLASSIFIER HEAD ==========\nclass CustomHead(nn.Module):\n    def __init__(self, in_features, num_classes=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\n# ========== 4. LOAD MODELS ==========\nprint(\"\\n[3/5] Loading trained models...\")\n\nmodel_configs = {\n    'resnet50': {\n        'model_fn': lambda: timm.create_model('resnet50', pretrained=False, num_classes=4),\n        'weight_file': 'resnet50_best_80_10_10.pth',\n        'target_layer': 'layer4.2',\n        'has_custom_head': True,\n        'num_features': 2048\n    },\n    'mobilenetv2': {\n        'model_fn': lambda: timm.create_model('mobilenetv2_100', pretrained=False, num_classes=4),\n        'weight_file': 'mobilenetv2_best_80_10_10.pth',\n        'target_layer': 'conv_head',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'efficientnetb0': {\n        'model_fn': lambda: timm.create_model('efficientnet_b0', pretrained=False, num_classes=4),\n        'weight_file': 'efficientnetb0_best_80_10_10.pth',\n        'target_layer': 'conv_head',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'vit': {\n        'model_fn': lambda: timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4),\n        'weight_file': 'vit_best_80_10_10.pth',\n        'target_layer': 'blocks.11.mlp',\n        'has_custom_head': False\n    },\n    'swin': {\n        'model_fn': lambda: timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4),\n        'weight_file': 'swin_best_80_10_10.pth',\n        'target_layer': 'layers.2',\n        'has_custom_head': False\n    }\n}\n\nmodels = {}\nfor model_name, cfg in model_configs.items():\n    try:\n        model = cfg['model_fn']()\n        \n        if cfg['has_custom_head']:\n            model.fc = CustomHead(cfg['num_features'], num_classes=4)\n        \n        weight_path = os.path.join(WEIGHTS_DIR, cfg['weight_file'])\n        if os.path.exists(weight_path):\n            model.load_state_dict(torch.load(weight_path, map_location=DEVICE), strict=False)\n            model.to(DEVICE)\n            model.eval()\n            models[model_name] = model\n            print(f\"✓ {model_name.upper()} loaded from {weight_path}\")\n        else:\n            print(f\"⚠ {model_name.upper()} weights not found at {weight_path}\")\n    except Exception as e:\n        print(f\"✗ Error loading {model_name}: {e}\")\n\nprint(f\"\\n✓ Total models loaded: {len(models)}\")\n\n# ========== 5a. STANDARD Grad-CAM FOR CNNs AND SWIN ==========\nclass GradCAM:\n    def __init__(self, model, target_layer_name):\n        self.model = model\n        self.target_layer_name = target_layer_name\n        self.gradients = None\n        self.activations = None\n        self.hooks = []\n        self._register_hooks()\n    \n    def _register_hooks(self):\n        def forward_hook(module, inp, out):\n            self.activations = out\n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0]\n        for name, module in self.model.named_modules():\n            if self.target_layer_name in name or name.endswith(self.target_layer_name):\n                self.hooks.append(module.register_forward_hook(forward_hook))\n                self.hooks.append(module.register_full_backward_hook(backward_hook))\n                break\n    \n    def generate_cam(self, input_tensor, class_idx=None):\n        output = self.model(input_tensor)\n        if class_idx is None:\n            class_idx = output.argmax(dim=1).item()\n        self.model.zero_grad()\n        score = output[0, class_idx]\n        score.backward(retain_graph=True)\n\n        grads = self.gradients.detach().cpu().numpy()[0]\n        acts = self.activations.detach().cpu().numpy()[0]\n        weights = grads.mean(axis=(1, 2))\n        cam = np.zeros(acts.shape[1:], dtype=np.float32)\n        for c, w in enumerate(weights):\n            cam += w * acts[c]\n        cam = np.maximum(cam, 0)\n        cam = cam / (cam.max() + 1e-8)\n        return cam, class_idx\n    \n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n\n# ========== 5b. ViT Attention Rollout CAM ==========\nclass ViTAttentionCAM:\n    def __init__(self, model):\n        self.model = model\n        self.attentions = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        self.hooks = []\n        def hook(module, input, output):\n            self.attentions.append(output)\n        for name, module in self.model.named_modules():\n            if \"attn.attn_drop\" in name or \"attn_drop\" in name:\n                self.hooks.append(module.register_forward_hook(hook))\n\n    def get_rollout_attention(self):\n        rollout = torch.eye(self.attentions[0].size(-1)).to(self.attentions[0].device)\n        for attn in self.attentions:\n            attn_heads_fused = attn.mean(dim=1)\n            attn_heads_fused += torch.eye(attn_heads_fused.size(-1)).to(attn_heads_fused.device)\n            attn_heads_fused /= attn_heads_fused.sum(dim=-1, keepdim=True)\n            rollout = torch.matmul(rollout, attn_heads_fused)\n        return rollout\n\n    def generate_cam(self, input_tensor):\n        self.attentions = []\n        _ = self.model(input_tensor)\n        rollout = self.get_rollout_attention()[0, 0, 1:]\n        size = int(np.sqrt(rollout.size(0)))\n        cam_map = rollout.reshape(size, size).cpu().numpy()\n        cam_map = (cam_map - cam_map.min()) / (cam_map.max() - cam_map.min() + 1e-8)\n        return cam_map\n    \n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n\nprint(\"✓ CAM classes ready\")\n\n# ========== 6. GENERATE CAMs ==========\nprint(\"\\n[5/5] Generating CAMs...\")\n\ncam_output_dir = os.path.join(OUTPUT_DIR, 'cam_visualizations')\nos.makedirs(cam_output_dir, exist_ok=True)\n\nall_cams = {}\n\nif len(models) == 0:\n    print(\"✗ No models loaded. Cannot generate CAMs.\")\nelse:\n    for model_name, model in tqdm(models.items(), desc=\"Processing models\"):\n        all_cams[model_name] = {}\n        if model_name == 'vit':\n            cam_engine = ViTAttentionCAM(model)\n            for cls_name in class_names:\n                all_cams[model_name][cls_name] = []\n                for img_idx, img_path in enumerate(test_images[cls_name]):\n                    try:\n                        img_tensor, img_rgb = preprocess_image(img_path)\n                        cam_map = cam_engine.generate_cam(img_tensor)\n                        cam_resized = cv2.resize(cam_map, (IMG_SIZE, IMG_SIZE))\n                        cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                        cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                        overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                        save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                        cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                    cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                        all_cams[model_name][cls_name].append({\n                            'path': img_path,\n                            'overlay': overlay,\n                            'pred_class': None\n                        })\n                    except Exception as e:\n                        pass\n            cam_engine.remove_hooks()\n            print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n        else:\n            target_layer = model_configs[model_name]['target_layer']\n            cam_engine = GradCAM(model, target_layer)\n            for cls_name in class_names:\n                all_cams[model_name][cls_name] = []\n                for img_idx, img_path in enumerate(test_images[cls_name]):\n                    try:\n                        img_tensor, img_rgb = preprocess_image(img_path)\n                        cam, pred_class = cam_engine.generate_cam(img_tensor)\n                        cam_resized = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n                        cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                        cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                        overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                        save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                        cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                    cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                        all_cams[model_name][cls_name].append({\n                            'path': img_path,\n                            'overlay': overlay,\n                            'pred_class': pred_class\n                        })\n                    except Exception as e:\n                        pass\n            cam_engine.remove_hooks()\n            print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n\n    print(f\"\\n✓ All CAM overlays saved to: {cam_output_dir}\")\n\n    # ========== 7. CREATE COMPARISON GRIDS ==========\n    print(\"\\n[6/6] Creating comparison grids...\")\n\n    for cls_name in class_names:\n        fig, axes = plt.subplots(1, len(models) + 1, figsize=(24, 4))\n        first_img_path = test_images[cls_name][0]\n        _, img_rgb = preprocess_image(first_img_path)\n        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n        axes[0].imshow(img_rgb_uint8, cmap='gray')\n        axes[0].set_title(f'{cls_name} - Original', fontsize=11, fontweight='bold')\n        axes[0].axis('off')\n        for i, (model_name, _) in enumerate(models.items()):\n            if len(all_cams[model_name][cls_name]) > 0:\n                overlay = all_cams[model_name][cls_name][0]['overlay']\n                axes[i+1].imshow(overlay)\n                axes[i+1].set_title(model_name.upper(), fontsize=11, fontweight='bold')\n            axes[i+1].axis('off')\n        plt.tight_layout()\n        plt.savefig(os.path.join(cam_output_dir, f'comparison_grid_{cls_name}.png'), dpi=300)\n        plt.close()\n        print(f\"✓ Saved comparison grid for {cls_name}\")\n\n# ========== 8. SUMMARY ==========\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: GRAD-CAM ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n\nif len(models) > 0:\n    print(f\"\\nModels processed: {list(models.keys())}\")\n    print(f\"Classes analyzed: {class_names}\")\n    print(f\"Images per class: {NUM_IMAGES_PER_CLASS}\")\n    total_cams = sum(len(all_cams[m][c]) for m in models.keys() for c in class_names)\n    print(f\"Total CAM visualizations: {total_cams}\")\n    print(f\"\\nOutput: {cam_output_dir}\")\n    print(f\"Files: individual cam_<model>_<class>_<idx>.png + comparison_grid_<class>.png\")\n    print(\"\\n✓ All CAM visualizations saved successfully!\")\nelse:\n    print(\"\\n✗ No models loaded. Check weights.\")\n\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:25:37.936367Z","iopub.execute_input":"2025-11-26T22:25:37.937015Z","iopub.status.idle":"2025-11-26T22:27:47.070781Z","shell.execute_reply.started":"2025-11-26T22:25:37.936992Z","shell.execute_reply":"2025-11-26T22:27:47.070117Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nNOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS INCLUDING ViT ATTENTION CAM\n================================================================================\n\n✓ Device: cuda\n✓ Test dataset: /kaggle/input/split-dataset/test\n✓ Weights directory: /kaggle/input/weight/weights\n✓ Output directory: /kaggle/working\n\n[1/5] Loading test images...\n  CNV: 200\n  DME: 200\n  DRUSEN: 200\n  NORMAL: 200\n✓ Total test images: 800\n\n[2/5] Setting up preprocessing...\n✓ Preprocessing ready\n\n[3/5] Loading trained models...\n✓ RESNET50 loaded from /kaggle/input/weight/weights/resnet50_best_80_10_10.pth\n✓ MOBILENETV2 loaded from /kaggle/input/weight/weights/mobilenetv2_best_80_10_10.pth\n✓ EFFICIENTNETB0 loaded from /kaggle/input/weight/weights/efficientnetb0_best_80_10_10.pth\n✓ VIT loaded from /kaggle/input/weight/weights/vit_best_80_10_10.pth\n✓ SWIN loaded from /kaggle/input/weight/weights/swin_best_80_10_10.pth\n\n✓ Total models loaded: 5\n✓ CAM classes ready\n\n[5/5] Generating CAMs...\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  20%|██        | 1/5 [00:25<01:41, 25.39s/it]","output_type":"stream"},{"name":"stdout","text":"✓ RESNET50: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  40%|████      | 2/5 [00:47<01:11, 23.69s/it]","output_type":"stream"},{"name":"stdout","text":"✓ MOBILENETV2: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  60%|██████    | 3/5 [01:16<00:52, 26.15s/it]","output_type":"stream"},{"name":"stdout","text":"✓ EFFICIENTNETB0: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  80%|████████  | 4/5 [01:28<00:20, 20.30s/it]","output_type":"stream"},{"name":"stdout","text":"✓ VIT: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models: 100%|██████████| 5/5 [02:02<00:00, 24.41s/it]","output_type":"stream"},{"name":"stdout","text":"✓ SWIN: 200 CAMs per class\n\n✓ All CAM overlays saved to: /kaggle/working/cam_visualizations\n\n[6/6] Creating comparison grids...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved comparison grid for CNV\n✓ Saved comparison grid for DME\n✓ Saved comparison grid for DRUSEN\n✓ Saved comparison grid for NORMAL\n\n================================================================================\nSUMMARY: GRAD-CAM ANALYSIS COMPLETE\n================================================================================\n\nModels processed: ['resnet50', 'mobilenetv2', 'efficientnetb0', 'vit', 'swin']\nClasses analyzed: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\nImages per class: 200\nTotal CAM visualizations: 3200\n\nOutput: /kaggle/working/cam_visualizations\nFiles: individual cam_<model>_<class>_<idx>.png + comparison_grid_<class>.png\n\n✓ All CAM visualizations saved successfully!\n================================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"model_vit = models['vit']\n\nprint(\"\\nNamed modules in ViT containing 'attn':\")\nfor name, module in model_vit.named_modules():\n    if 'attn' in name:\n        print(name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:29:04.604542Z","iopub.execute_input":"2025-11-26T22:29:04.604804Z","iopub.status.idle":"2025-11-26T22:29:04.612062Z","shell.execute_reply.started":"2025-11-26T22:29:04.604780Z","shell.execute_reply":"2025-11-26T22:29:04.611511Z"}},"outputs":[{"name":"stdout","text":"\nNamed modules in ViT containing 'attn':\nblocks.0.attn\nblocks.0.attn.qkv\nblocks.0.attn.q_norm\nblocks.0.attn.k_norm\nblocks.0.attn.attn_drop\nblocks.0.attn.norm\nblocks.0.attn.proj\nblocks.0.attn.proj_drop\nblocks.1.attn\nblocks.1.attn.qkv\nblocks.1.attn.q_norm\nblocks.1.attn.k_norm\nblocks.1.attn.attn_drop\nblocks.1.attn.norm\nblocks.1.attn.proj\nblocks.1.attn.proj_drop\nblocks.2.attn\nblocks.2.attn.qkv\nblocks.2.attn.q_norm\nblocks.2.attn.k_norm\nblocks.2.attn.attn_drop\nblocks.2.attn.norm\nblocks.2.attn.proj\nblocks.2.attn.proj_drop\nblocks.3.attn\nblocks.3.attn.qkv\nblocks.3.attn.q_norm\nblocks.3.attn.k_norm\nblocks.3.attn.attn_drop\nblocks.3.attn.norm\nblocks.3.attn.proj\nblocks.3.attn.proj_drop\nblocks.4.attn\nblocks.4.attn.qkv\nblocks.4.attn.q_norm\nblocks.4.attn.k_norm\nblocks.4.attn.attn_drop\nblocks.4.attn.norm\nblocks.4.attn.proj\nblocks.4.attn.proj_drop\nblocks.5.attn\nblocks.5.attn.qkv\nblocks.5.attn.q_norm\nblocks.5.attn.k_norm\nblocks.5.attn.attn_drop\nblocks.5.attn.norm\nblocks.5.attn.proj\nblocks.5.attn.proj_drop\nblocks.6.attn\nblocks.6.attn.qkv\nblocks.6.attn.q_norm\nblocks.6.attn.k_norm\nblocks.6.attn.attn_drop\nblocks.6.attn.norm\nblocks.6.attn.proj\nblocks.6.attn.proj_drop\nblocks.7.attn\nblocks.7.attn.qkv\nblocks.7.attn.q_norm\nblocks.7.attn.k_norm\nblocks.7.attn.attn_drop\nblocks.7.attn.norm\nblocks.7.attn.proj\nblocks.7.attn.proj_drop\nblocks.8.attn\nblocks.8.attn.qkv\nblocks.8.attn.q_norm\nblocks.8.attn.k_norm\nblocks.8.attn.attn_drop\nblocks.8.attn.norm\nblocks.8.attn.proj\nblocks.8.attn.proj_drop\nblocks.9.attn\nblocks.9.attn.qkv\nblocks.9.attn.q_norm\nblocks.9.attn.k_norm\nblocks.9.attn.attn_drop\nblocks.9.attn.norm\nblocks.9.attn.proj\nblocks.9.attn.proj_drop\nblocks.10.attn\nblocks.10.attn.qkv\nblocks.10.attn.q_norm\nblocks.10.attn.k_norm\nblocks.10.attn.attn_drop\nblocks.10.attn.norm\nblocks.10.attn.proj\nblocks.10.attn.proj_drop\nblocks.11.attn\nblocks.11.attn.qkv\nblocks.11.attn.q_norm\nblocks.11.attn.k_norm\nblocks.11.attn.attn_drop\nblocks.11.attn.norm\nblocks.11.attn.proj\nblocks.11.attn.proj_drop\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport timm\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"NOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS INCLUDING ViT ATTENTION CAM\")\nprint(\"=\"*80)\n\n# ========== CONFIG ==========\nTEST_DIR = '/kaggle/input/split-dataset/test'\nWEIGHTS_DIR = '/kaggle/input/weight/weights'\nOUTPUT_DIR = '/kaggle/working'\nIMG_SIZE = 224\nNUM_IMAGES_PER_CLASS = 200\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n✓ Device: {DEVICE}\")\nprint(f\"✓ Test dataset: {TEST_DIR}\")\nprint(f\"✓ Weights directory: {WEIGHTS_DIR}\")\nprint(f\"✓ Output directory: {OUTPUT_DIR}\")\n\n# ========== 1. LOAD TEST IMAGES ==========\nprint(\"\\n[1/5] Loading test images...\")\nclass_names = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\ntest_images = {cls: [] for cls in class_names}\n\nfor cls in class_names:\n    cls_path = os.path.join(TEST_DIR, cls)\n    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    sampled = random.sample(images, min(NUM_IMAGES_PER_CLASS, len(images)))\n    test_images[cls] = [os.path.join(cls_path, img) for img in sampled]\n\nfor cls in class_names:\n    print(f\"  {cls}: {len(test_images[cls])}\")\ntotal_images = sum(len(v) for v in test_images.values())\nprint(f\"✓ Total test images: {total_images}\")\n\n# ========== 2. PREPROCESS ==========\nprint(\"\\n[2/5] Setting up preprocessing...\")\ndef preprocess_image(img_path):\n    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    image_norm = image_rgb.astype(np.float32) / 255.0\n    image_norm = (image_norm - 0.5) / 0.5\n    img_tensor = torch.from_numpy(image_norm).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n    img_tensor.requires_grad_(True)  # Enable gradients for CAM\n    return img_tensor, image_rgb\n\nprint(\"✓ Preprocessing ready\")\n\n# ========== 3. CUSTOM CNN CLASSIFIER HEAD ==========\nclass CustomHead(nn.Module):\n    def __init__(self, in_features, num_classes=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n# ========== 4. LOAD MODELS ==========\nprint(\"\\n[3/5] Loading trained models...\")\nmodel_configs = {\n    'resnet50': {\n        'model_fn': lambda: timm.create_model('resnet50', pretrained=False, num_classes=4),\n        'weight_file': 'resnet50_best_80_10_10.pth',\n        'target_layer': 'layer4.2',\n        'has_custom_head': True,\n        'num_features': 2048\n    },\n    'mobilenetv2': {\n        'model_fn': lambda: timm.create_model('mobilenetv2_100', pretrained=False, num_classes=4),\n        'weight_file': 'mobilenetv2_best_80_10_10.pth',\n        'target_layer': 'conv_head',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'efficientnetb0': {\n        'model_fn': lambda: timm.create_model('efficientnet_b0', pretrained=False, num_classes=4),\n        'weight_file': 'efficientnetb0_best_80_10_10.pth',\n        'target_layer': 'conv_head',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'vit': {\n        'model_fn': lambda: timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4),\n        'weight_file': 'vit_best_80_10_10.pth',\n        'target_layer': 'blocks.attn',  # generalized target; hooks will be installed on all 'attn' modules\n        'has_custom_head': False\n    },\n    'swin': {\n        'model_fn': lambda: timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4),\n        'weight_file': 'swin_best_80_10_10.pth',\n        'target_layer': 'layers.2',\n        'has_custom_head': False\n    }\n}\n\nmodels = {}\nfor model_name, cfg in model_configs.items():\n    try:\n        model = cfg['model_fn']()\n\n        if cfg['has_custom_head']:\n            model.fc = CustomHead(cfg['num_features'], num_classes=4)\n\n        weight_path = os.path.join(WEIGHTS_DIR, cfg['weight_file'])\n        if os.path.exists(weight_path):\n            model.load_state_dict(torch.load(weight_path, map_location=DEVICE), strict=False)\n            model.to(DEVICE)\n            model.eval()\n            models[model_name] = model\n            print(f\"✓ {model_name.upper()} loaded from {weight_path}\")\n        else:\n            print(f\"⚠ {model_name.upper()} weights not found at {weight_path}\")\n    except Exception as e:\n        print(f\"✗ Error loading {model_name}: {e}\")\n\nprint(f\"\\n✓ Total models loaded: {len(models)}\")\n\n# ========== 5a. STANDARD Grad-CAM FOR CNNs AND SWIN ==========\nclass GradCAM:\n    def __init__(self, model, target_layer_name):\n        self.model = model\n        self.target_layer_name = target_layer_name\n        self.gradients = None\n        self.activations = None\n        self.hooks = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def forward_hook(module, inp, out):\n            self.activations = out\n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0]\n        for name, module in self.model.named_modules():\n            if self.target_layer_name in name or name.endswith(self.target_layer_name):\n                self.hooks.append(module.register_forward_hook(forward_hook))\n                self.hooks.append(module.register_full_backward_hook(backward_hook))\n                break\n\n    def generate_cam(self, input_tensor, class_idx=None):\n        output = self.model(input_tensor)\n        if class_idx is None:\n            class_idx = output.argmax(dim=1).item()\n        self.model.zero_grad()\n        score = output[0, class_idx]\n        score.backward(retain_graph=True)\n\n        grads = self.gradients.detach().cpu().numpy()[0]\n        acts = self.activations.detach().cpu().numpy()[0]\n        weights = grads.mean(axis=(1, 2))\n        cam = np.zeros(acts.shape[1:], dtype=np.float32)\n        for c, w in enumerate(weights):\n            cam += w * acts[c]\n        cam = np.maximum(cam, 0)\n        cam = cam / (cam.max() + 1e-8)\n        return cam, class_idx\n\n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n\n# ========== 5b. ViT Attention Rollout CAM ==========\nclass ViTAttentionCAM:\n    def __init__(self, model):\n        self.model = model\n        self.attentions = []\n        self.hooks = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def hook(module, input, output):\n            self.attentions.append(output.detach())\n        for name, module in self.model.named_modules():\n            if name.endswith('attn'):\n                self.hooks.append(module.register_forward_hook(hook))\n\n    def get_rollout_attention(self):\n        if not self.attentions:\n            raise RuntimeError(\"No attentions captured. Check hooks.\")\n        rollout = torch.eye(self.attentions[0].size(-1)).to(self.attentions[0].device)\n        for attn in self.attentions:\n            attn_heads_fused = attn.mean(dim=1)\n            attn_heads_fused += torch.eye(attn_heads_fused.size(-1)).to(attn_heads_fused.device)\n            attn_heads_fused /= attn_heads_fused.sum(dim=-1, keepdim=True)\n            rollout = torch.matmul(rollout, attn_heads_fused)\n        return rollout\n\n    def generate_cam(self, input_tensor):\n        self.attentions.clear()\n        _ = self.model(input_tensor)\n        rollout = self.get_rollout_attention()\n        cam = rollout[0, 0, 1:].reshape(int(np.sqrt(rollout.size(-1) - 1)),\n                                        int(np.sqrt(rollout.size(-1) - 1))).cpu().numpy()\n        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n        return cam\n\n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n        self.hooks = []\n\nprint(\"✓ CAM classes ready\")\n\n# ========== 6. GENERATE CAMs ==========\nprint(\"\\n[5/5] Generating CAMs...\")\n\ncam_output_dir = os.path.join(OUTPUT_DIR, 'cam_visualizations')\nos.makedirs(cam_output_dir, exist_ok=True)\n\nall_cams = {}\n\nif len(models) == 0:\n    print(\"✗ No models loaded. Cannot generate CAMs.\")\nelse:\n    for model_name, model in tqdm(models.items(), desc=\"Processing models\"):\n        all_cams[model_name] = {}\n        if model_name == 'vit':\n            cam_engine = ViTAttentionCAM(model)\n            for cls_name in class_names:\n                all_cams[model_name][cls_name] = []\n                for img_idx, img_path in enumerate(test_images[cls_name]):\n                    try:\n                        img_tensor, img_rgb = preprocess_image(img_path)\n                        cam_map = cam_engine.generate_cam(img_tensor)\n                        cam_resized = cv2.resize(cam_map, (IMG_SIZE, IMG_SIZE))\n                        cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                        cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                        overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                        save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                        cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                    cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                        all_cams[model_name][cls_name].append({\n                            'path': img_path,\n                            'overlay': overlay,\n                            'pred_class': None\n                        })\n                    except Exception:\n                        pass\n            cam_engine.remove_hooks()\n            print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n        else:\n            target_layer = model_configs[model_name]['target_layer']\n            cam_engine = GradCAM(model, target_layer)\n            for cls_name in class_names:\n                all_cams[model_name][cls_name] = []\n                for img_idx, img_path in enumerate(test_images[cls_name]):\n                    try:\n                        img_tensor, img_rgb = preprocess_image(img_path)\n                        cam, pred_class = cam_engine.generate_cam(img_tensor)\n                        cam_resized = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n                        cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                        cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                        overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                        save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                        cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                    cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                        all_cams[model_name][cls_name].append({\n                            'path': img_path,\n                            'overlay': overlay,\n                            'pred_class': pred_class\n                        })\n                    except Exception:\n                        pass\n            cam_engine.remove_hooks()\n            print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n\n    print(f\"\\n✓ All CAM overlays saved to: {cam_output_dir}\")\n\n# ========== 7. CREATE COMPARISON GRIDS ==========\nprint(\"\\n[6/6] Creating comparison grids...\")\nfor cls_name in class_names:\n    fig, axes = plt.subplots(1, len(models) + 1, figsize=(24, 4))\n    first_img_path = test_images[cls_name][0]\n    _, img_rgb = preprocess_image(first_img_path)\n    img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n    axes[0].imshow(img_rgb_uint8, cmap='gray')\n    axes[0].set_title(f'{cls_name} - Original', fontsize=11, fontweight='bold')\n    axes[0].axis('off')\n    for i, (model_name, _) in enumerate(models.items()):\n        if len(all_cams[model_name][cls_name]) > 0:\n            overlay = all_cams[model_name][cls_name][0]['overlay']\n            axes[i+1].imshow(overlay)\n            axes[i+1].set_title(model_name.upper(), fontsize=11, fontweight='bold')\n        axes[i+1].axis('off')\n    plt.tight_layout()\n    plt.savefig(os.path.join(cam_output_dir, f'comparison_grid_{cls_name}.png'), dpi=300)\n    plt.close()\n    print(f\"✓ Saved comparison grid for {cls_name}\")\n\n# ========== 8. SUMMARY ==========\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: GRAD-CAM ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n\nif len(models) > 0:\n    print(f\"\\nModels processed: {list(models.keys())}\")\n    print(f\"Classes analyzed: {class_names}\")\n    print(f\"Images per class: {NUM_IMAGES_PER_CLASS}\")\n    total_cams = sum(len(all_cams[m][c]) for m in models.keys() for c in class_names)\n    print(f\"Total CAM visualizations: {total_cams}\")\n    print(f\"\\nOutput: {cam_output_dir}\")\n    print(f\"Files: individual cam_<model>_<class>_<idx>.png + comparison_grid_<class>.png\")\n    print(\"\\n✓ All CAM visualizations saved successfully!\")\nelse:\n    print(\"\\n✗ No models loaded. Check weights.\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:30:47.569011Z","iopub.execute_input":"2025-11-26T22:30:47.569815Z","iopub.status.idle":"2025-11-26T22:32:57.117432Z","shell.execute_reply.started":"2025-11-26T22:30:47.569773Z","shell.execute_reply":"2025-11-26T22:32:57.116811Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nNOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS INCLUDING ViT ATTENTION CAM\n================================================================================\n\n✓ Device: cuda\n✓ Test dataset: /kaggle/input/split-dataset/test\n✓ Weights directory: /kaggle/input/weight/weights\n✓ Output directory: /kaggle/working\n\n[1/5] Loading test images...\n  CNV: 200\n  DME: 200\n  DRUSEN: 200\n  NORMAL: 200\n✓ Total test images: 800\n\n[2/5] Setting up preprocessing...\n✓ Preprocessing ready\n\n[3/5] Loading trained models...\n✓ RESNET50 loaded from /kaggle/input/weight/weights/resnet50_best_80_10_10.pth\n✓ MOBILENETV2 loaded from /kaggle/input/weight/weights/mobilenetv2_best_80_10_10.pth\n✓ EFFICIENTNETB0 loaded from /kaggle/input/weight/weights/efficientnetb0_best_80_10_10.pth\n✓ VIT loaded from /kaggle/input/weight/weights/vit_best_80_10_10.pth\n✓ SWIN loaded from /kaggle/input/weight/weights/swin_best_80_10_10.pth\n\n✓ Total models loaded: 5\n✓ CAM classes ready\n\n[5/5] Generating CAMs...\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  20%|██        | 1/5 [00:24<01:39, 24.78s/it]","output_type":"stream"},{"name":"stdout","text":"✓ RESNET50: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  40%|████      | 2/5 [00:47<01:10, 23.35s/it]","output_type":"stream"},{"name":"stdout","text":"✓ MOBILENETV2: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  60%|██████    | 3/5 [01:15<00:51, 25.72s/it]","output_type":"stream"},{"name":"stdout","text":"✓ EFFICIENTNETB0: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  80%|████████  | 4/5 [01:29<00:20, 20.96s/it]","output_type":"stream"},{"name":"stdout","text":"✓ VIT: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models: 100%|██████████| 5/5 [02:02<00:00, 24.57s/it]","output_type":"stream"},{"name":"stdout","text":"✓ SWIN: 200 CAMs per class\n\n✓ All CAM overlays saved to: /kaggle/working/cam_visualizations\n\n[6/6] Creating comparison grids...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved comparison grid for CNV\n✓ Saved comparison grid for DME\n✓ Saved comparison grid for DRUSEN\n✓ Saved comparison grid for NORMAL\n\n================================================================================\nSUMMARY: GRAD-CAM ANALYSIS COMPLETE\n================================================================================\n\nModels processed: ['resnet50', 'mobilenetv2', 'efficientnetb0', 'vit', 'swin']\nClasses analyzed: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\nImages per class: 200\nTotal CAM visualizations: 3200\n\nOutput: /kaggle/working/cam_visualizations\nFiles: individual cam_<model>_<class>_<idx>.png + comparison_grid_<class>.png\n\n✓ All CAM visualizations saved successfully!\n================================================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model_vit = models['vit']\nprint(\"ViT named modules with 'attn' or 'drop' in name:\")\nfor name, module in model_vit.named_modules():\n    if 'attn' in name or 'drop' in name:\n        print(name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:33:34.031613Z","iopub.execute_input":"2025-11-26T22:33:34.031900Z","iopub.status.idle":"2025-11-26T22:33:34.039722Z","shell.execute_reply.started":"2025-11-26T22:33:34.031880Z","shell.execute_reply":"2025-11-26T22:33:34.039138Z"}},"outputs":[{"name":"stdout","text":"ViT named modules with 'attn' or 'drop' in name:\npos_drop\npatch_drop\nblocks.0.attn\nblocks.0.attn.qkv\nblocks.0.attn.q_norm\nblocks.0.attn.k_norm\nblocks.0.attn.attn_drop\nblocks.0.attn.norm\nblocks.0.attn.proj\nblocks.0.attn.proj_drop\nblocks.0.drop_path1\nblocks.0.mlp.drop1\nblocks.0.mlp.drop2\nblocks.0.drop_path2\nblocks.1.attn\nblocks.1.attn.qkv\nblocks.1.attn.q_norm\nblocks.1.attn.k_norm\nblocks.1.attn.attn_drop\nblocks.1.attn.norm\nblocks.1.attn.proj\nblocks.1.attn.proj_drop\nblocks.1.drop_path1\nblocks.1.mlp.drop1\nblocks.1.mlp.drop2\nblocks.1.drop_path2\nblocks.2.attn\nblocks.2.attn.qkv\nblocks.2.attn.q_norm\nblocks.2.attn.k_norm\nblocks.2.attn.attn_drop\nblocks.2.attn.norm\nblocks.2.attn.proj\nblocks.2.attn.proj_drop\nblocks.2.drop_path1\nblocks.2.mlp.drop1\nblocks.2.mlp.drop2\nblocks.2.drop_path2\nblocks.3.attn\nblocks.3.attn.qkv\nblocks.3.attn.q_norm\nblocks.3.attn.k_norm\nblocks.3.attn.attn_drop\nblocks.3.attn.norm\nblocks.3.attn.proj\nblocks.3.attn.proj_drop\nblocks.3.drop_path1\nblocks.3.mlp.drop1\nblocks.3.mlp.drop2\nblocks.3.drop_path2\nblocks.4.attn\nblocks.4.attn.qkv\nblocks.4.attn.q_norm\nblocks.4.attn.k_norm\nblocks.4.attn.attn_drop\nblocks.4.attn.norm\nblocks.4.attn.proj\nblocks.4.attn.proj_drop\nblocks.4.drop_path1\nblocks.4.mlp.drop1\nblocks.4.mlp.drop2\nblocks.4.drop_path2\nblocks.5.attn\nblocks.5.attn.qkv\nblocks.5.attn.q_norm\nblocks.5.attn.k_norm\nblocks.5.attn.attn_drop\nblocks.5.attn.norm\nblocks.5.attn.proj\nblocks.5.attn.proj_drop\nblocks.5.drop_path1\nblocks.5.mlp.drop1\nblocks.5.mlp.drop2\nblocks.5.drop_path2\nblocks.6.attn\nblocks.6.attn.qkv\nblocks.6.attn.q_norm\nblocks.6.attn.k_norm\nblocks.6.attn.attn_drop\nblocks.6.attn.norm\nblocks.6.attn.proj\nblocks.6.attn.proj_drop\nblocks.6.drop_path1\nblocks.6.mlp.drop1\nblocks.6.mlp.drop2\nblocks.6.drop_path2\nblocks.7.attn\nblocks.7.attn.qkv\nblocks.7.attn.q_norm\nblocks.7.attn.k_norm\nblocks.7.attn.attn_drop\nblocks.7.attn.norm\nblocks.7.attn.proj\nblocks.7.attn.proj_drop\nblocks.7.drop_path1\nblocks.7.mlp.drop1\nblocks.7.mlp.drop2\nblocks.7.drop_path2\nblocks.8.attn\nblocks.8.attn.qkv\nblocks.8.attn.q_norm\nblocks.8.attn.k_norm\nblocks.8.attn.attn_drop\nblocks.8.attn.norm\nblocks.8.attn.proj\nblocks.8.attn.proj_drop\nblocks.8.drop_path1\nblocks.8.mlp.drop1\nblocks.8.mlp.drop2\nblocks.8.drop_path2\nblocks.9.attn\nblocks.9.attn.qkv\nblocks.9.attn.q_norm\nblocks.9.attn.k_norm\nblocks.9.attn.attn_drop\nblocks.9.attn.norm\nblocks.9.attn.proj\nblocks.9.attn.proj_drop\nblocks.9.drop_path1\nblocks.9.mlp.drop1\nblocks.9.mlp.drop2\nblocks.9.drop_path2\nblocks.10.attn\nblocks.10.attn.qkv\nblocks.10.attn.q_norm\nblocks.10.attn.k_norm\nblocks.10.attn.attn_drop\nblocks.10.attn.norm\nblocks.10.attn.proj\nblocks.10.attn.proj_drop\nblocks.10.drop_path1\nblocks.10.mlp.drop1\nblocks.10.mlp.drop2\nblocks.10.drop_path2\nblocks.11.attn\nblocks.11.attn.qkv\nblocks.11.attn.q_norm\nblocks.11.attn.k_norm\nblocks.11.attn.attn_drop\nblocks.11.attn.norm\nblocks.11.attn.proj\nblocks.11.attn.proj_drop\nblocks.11.drop_path1\nblocks.11.mlp.drop1\nblocks.11.mlp.drop2\nblocks.11.drop_path2\nhead_drop\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport timm\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"NOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS INCLUDING REFINED ViT ATTENTION CAM\")\nprint(\"=\"*80)\n\n# ========== CONFIG ==========\nTEST_DIR = '/kaggle/input/split-dataset/test'\nWEIGHTS_DIR = '/kaggle/input/weight/weights'\nOUTPUT_DIR = '/kaggle/working'\nIMG_SIZE = 224\nNUM_IMAGES_PER_CLASS = 200\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n✓ Device: {DEVICE}\")\nprint(f\"✓ Test dataset: {TEST_DIR}\")\nprint(f\"✓ Weights directory: {WEIGHTS_DIR}\")\nprint(f\"✓ Output directory: {OUTPUT_DIR}\")\n\n# ========== 1. LOAD TEST IMAGES ==========\nprint(\"\\n[1/5] Loading test images...\")\nclass_names = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\ntest_images = {cls: [] for cls in class_names}\n\nfor cls in class_names:\n    cls_path = os.path.join(TEST_DIR, cls)\n    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    sampled = random.sample(images, min(NUM_IMAGES_PER_CLASS, len(images)))\n    test_images[cls] = [os.path.join(cls_path, img) for img in sampled]\n\nfor cls in class_names:\n    print(f\"  {cls}: {len(test_images[cls])}\")\ntotal_images = sum(len(v) for v in test_images.values())\nprint(f\"✓ Total test images: {total_images}\")\n\n# ========== 2. PREPROCESS ==========\nprint(\"\\n[2/5] Setting up preprocessing...\")\ndef preprocess_image(img_path):\n    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    image_norm = image_rgb.astype(np.float32) / 255.0\n    image_norm = (image_norm - 0.5) / 0.5\n    img_tensor = torch.from_numpy(image_norm).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n    img_tensor.requires_grad_(True)\n    return img_tensor, image_rgb\n\nprint(\"✓ Preprocessing ready\")\n\n# ========== 3. CUSTOM CNN CLASSIFIER HEAD ==========\nclass CustomHead(nn.Module):\n    def __init__(self, in_features, num_classes=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n    def forward(self, x):\n        return self.fc(x)\n\n# ========== 4. LOAD MODELS ==========\nprint(\"\\n[3/5] Loading trained models...\")\nmodel_configs = {\n    'resnet50': {\n        'model_fn': lambda: timm.create_model('resnet50', pretrained=False, num_classes=4),\n        'weight_file': 'resnet50_best_80_10_10.pth',\n        'target_layer': 'layer4.2',\n        'has_custom_head': True,\n        'num_features': 2048\n    },\n    'mobilenetv2': {\n        'model_fn': lambda: timm.create_model('mobilenetv2_100', pretrained=False, num_classes=4),\n        'weight_file': 'mobilenetv2_best_80_10_10.pth',\n        'target_layer': 'conv_head',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'efficientnetb0': {\n        'model_fn': lambda: timm.create_model('efficientnet_b0', pretrained=False, num_classes=4),\n        'weight_file': 'efficientnetb0_best_80_10_10.pth',\n        'target_layer': 'conv_head',\n        'has_custom_head': True,\n        'num_features': 1280\n    },\n    'vit': {\n        'model_fn': lambda: timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=4),\n        'weight_file': 'vit_best_80_10_10.pth',\n        'target_layer': 'blocks.attn',  # generalized for ViT hooks\n        'has_custom_head': False\n    },\n    'swin': {\n        'model_fn': lambda: timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4),\n        'weight_file': 'swin_best_80_10_10.pth',\n        'target_layer': 'layers.2',\n        'has_custom_head': False\n    }\n}\n\nmodels = {}\nfor model_name, cfg in model_configs.items():\n    try:\n        model = cfg['model_fn']()\n        if cfg['has_custom_head']:\n            model.fc = CustomHead(cfg['num_features'], num_classes=4)\n        weight_path = os.path.join(WEIGHTS_DIR, cfg['weight_file'])\n        if os.path.exists(weight_path):\n            model.load_state_dict(torch.load(weight_path, map_location=DEVICE), strict=False)\n            model.to(DEVICE)\n            model.eval()\n            models[model_name] = model\n            print(f\"✓ {model_name.upper()} loaded from {weight_path}\")\n        else:\n            print(f\"⚠ {model_name.upper()} weights not found at {weight_path}\")\n    except Exception as e:\n        print(f\"✗ Error loading {model_name}: {e}\")\n\nprint(f\"\\n✓ Total models loaded: {len(models)}\")\n\n# ========== 5a. GradCAM for CNNs and Swin ==========\nclass GradCAM:\n    def __init__(self, model, target_layer_name):\n        self.model = model\n        self.target_layer_name = target_layer_name\n        self.gradients = None\n        self.activations = None\n        self.hooks = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def forward_hook(module, inp, out):\n            self.activations = out\n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0]\n        for name, module in self.model.named_modules():\n            if self.target_layer_name in name or name.endswith(self.target_layer_name):\n                self.hooks.append(module.register_forward_hook(forward_hook))\n                self.hooks.append(module.register_full_backward_hook(backward_hook))\n                break\n\n    def generate_cam(self, input_tensor, class_idx=None):\n        output = self.model(input_tensor)\n        if class_idx is None:\n            class_idx = output.argmax(dim=1).item()\n        self.model.zero_grad()\n        score = output[0, class_idx]\n        score.backward(retain_graph=True)\n        grads = self.gradients.detach().cpu().numpy()[0]\n        acts = self.activations.detach().cpu().numpy()[0]\n        weights = grads.mean(axis=(1, 2))\n        cam = np.zeros(acts.shape[1:], dtype=np.float32)\n        for c, w in enumerate(weights):\n            cam += w * acts[c]\n        cam = np.maximum(cam, 0)\n        cam = cam / (cam.max() + 1e-8)\n        return cam, class_idx\n\n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n\n# ========== 5b. ViT Attention Rollout CAM ==========\nclass ViTAttentionCAM:\n    def __init__(self, model):\n        self.model = model\n        self.attentions = []\n        self.hooks = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def hook(module, input, output):\n            self.attentions.append(output.detach())\n        for name, module in self.model.named_modules():\n            if name.count('attn') == 1 and 'blocks' in name and name.endswith('attn'):\n                self.hooks.append(module.register_forward_hook(hook))\n\n    def get_rollout_attention(self):\n        if not self.attentions:\n            raise RuntimeError(\"No attentions captured. Check hooks.\")\n        rollout = torch.eye(self.attentions[0].size(-1)).to(self.attentions[0].device)\n        for attn in self.attentions:\n            attn_heads_fused = attn.mean(dim=1)\n            attn_heads_fused += torch.eye(attn_heads_fused.size(-1)).to(attn_heads_fused.device)\n            attn_heads_fused /= attn_heads_fused.sum(dim=-1, keepdim=True)\n            rollout = torch.matmul(rollout, attn_heads_fused)\n        return rollout\n\n    def generate_cam(self, input_tensor):\n        self.attentions.clear()\n        _ = self.model(input_tensor)\n        rollout = self.get_rollout_attention()\n        cam = rollout[0, 0, 1:]\n        size = int(np.sqrt(cam.size(0)))\n        cam_map = cam.reshape(size, size).cpu().numpy()\n        cam_map = (cam_map - cam_map.min()) / (cam_map.max() - cam_map.min() + 1e-8)\n        return cam_map\n\n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n        self.hooks = []\n\nprint(\"✓ CAM classes ready\")\n\n# ========== 6. GENERATE CAMs ==========\nprint(\"\\n[5/5] Generating CAMs...\")\n\ncam_output_dir = os.path.join(OUTPUT_DIR, 'cam_visualizations')\nos.makedirs(cam_output_dir, exist_ok=True)\n\nall_cams = {}\n\nif len(models) == 0:\n    print(\"✗ No models loaded. Cannot generate CAMs.\")\nelse:\n    for model_name, model in tqdm(models.items(), desc=\"Processing models\"):\n        all_cams[model_name] = {}\n        if model_name == 'vit':\n            cam_engine = ViTAttentionCAM(model)\n            for cls_name in class_names:\n                all_cams[model_name][cls_name] = []\n                for img_idx, img_path in enumerate(test_images[cls_name]):\n                    try:\n                        img_tensor, img_rgb = preprocess_image(img_path)\n                        cam_map = cam_engine.generate_cam(img_tensor)\n                        cam_resized = cv2.resize(cam_map, (IMG_SIZE, IMG_SIZE))\n                        cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                        cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                        overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                        save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                        cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                    cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                        all_cams[model_name][cls_name].append({\n                            'path': img_path,\n                            'overlay': overlay,\n                            'pred_class': None\n                        })\n                    except Exception:\n                        pass\n            cam_engine.remove_hooks()\n            print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n        else:\n            target_layer = model_configs[model_name]['target_layer']\n            cam_engine = GradCAM(model, target_layer)\n            for cls_name in class_names:\n                all_cams[model_name][cls_name] = []\n                for img_idx, img_path in enumerate(test_images[cls_name]):\n                    try:\n                        img_tensor, img_rgb = preprocess_image(img_path)\n                        cam, pred_class = cam_engine.generate_cam(img_tensor)\n                        cam_resized = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n                        cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                        cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                        img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                        overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                        save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                        cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                    cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                        all_cams[model_name][cls_name].append({\n                            'path': img_path,\n                            'overlay': overlay,\n                            'pred_class': pred_class\n                        })\n                    except Exception:\n                        pass\n            cam_engine.remove_hooks()\n            print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n\n    print(f\"\\n✓ All CAM overlays saved to: {cam_output_dir}\")\n\n# ========== 7. CREATE COMPARISON GRIDS ==========\nprint(\"\\n[6/6] Creating comparison grids...\")\nfor cls_name in class_names:\n    fig, axes = plt.subplots(1, len(models) + 1, figsize=(24, 4))\n    first_img_path = test_images[cls_name][0]\n    _, img_rgb = preprocess_image(first_img_path)\n    img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n    axes[0].imshow(img_rgb_uint8, cmap='gray')\n    axes[0].set_title(f'{cls_name} - Original', fontsize=11, fontweight='bold')\n    axes[0].axis('off')\n    for i, (model_name, _) in enumerate(models.items()):\n        if len(all_cams[model_name][cls_name]) > 0:\n            overlay = all_cams[model_name][cls_name][0]['overlay']\n            axes[i+1].imshow(overlay)\n            axes[i+1].set_title(model_name.upper(), fontsize=11, fontweight='bold')\n        axes[i+1].axis('off')\n    plt.tight_layout()\n    plt.savefig(os.path.join(cam_output_dir, f'comparison_grid_{cls_name}.png'), dpi=300)\n    plt.close()\n    print(f\"✓ Saved comparison grid for {cls_name}\")\n\n# ========== 8. SUMMARY ==========\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: GRAD-CAM ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n\nif len(models) > 0:\n    print(f\"\\nModels processed: {list(models.keys())}\")\n    print(f\"Classes analyzed: {class_names}\")\n    print(f\"Images per class: {NUM_IMAGES_PER_CLASS}\")\n    total_cams = sum(len(all_cams[m][c]) for m in models.keys() for c in class_names)\n    print(f\"Total CAM visualizations: {total_cams}\")\n    print(f\"\\nOutput: {cam_output_dir}\")\n    print(f\"Files: individual cam_<model>_<class>_<idx>.png + comparison_grid_<class>.png\")\n    print(\"\\n✓ All CAM visualizations saved successfully!\")\nelse:\n    print(\"\\n✗ No models loaded. Check weights.\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:34:48.406266Z","iopub.execute_input":"2025-11-26T22:34:48.406868Z","iopub.status.idle":"2025-11-26T22:36:58.678434Z","shell.execute_reply.started":"2025-11-26T22:34:48.406846Z","shell.execute_reply":"2025-11-26T22:36:58.677649Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nNOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - ALL 5 MODELS INCLUDING REFINED ViT ATTENTION CAM\n================================================================================\n\n✓ Device: cuda\n✓ Test dataset: /kaggle/input/split-dataset/test\n✓ Weights directory: /kaggle/input/weight/weights\n✓ Output directory: /kaggle/working\n\n[1/5] Loading test images...\n  CNV: 200\n  DME: 200\n  DRUSEN: 200\n  NORMAL: 200\n✓ Total test images: 800\n\n[2/5] Setting up preprocessing...\n✓ Preprocessing ready\n\n[3/5] Loading trained models...\n✓ RESNET50 loaded from /kaggle/input/weight/weights/resnet50_best_80_10_10.pth\n✓ MOBILENETV2 loaded from /kaggle/input/weight/weights/mobilenetv2_best_80_10_10.pth\n✓ EFFICIENTNETB0 loaded from /kaggle/input/weight/weights/efficientnetb0_best_80_10_10.pth\n✓ VIT loaded from /kaggle/input/weight/weights/vit_best_80_10_10.pth\n✓ SWIN loaded from /kaggle/input/weight/weights/swin_best_80_10_10.pth\n\n✓ Total models loaded: 5\n✓ CAM classes ready\n\n[5/5] Generating CAMs...\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  20%|██        | 1/5 [00:25<01:40, 25.08s/it]","output_type":"stream"},{"name":"stdout","text":"✓ RESNET50: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  40%|████      | 2/5 [00:47<01:10, 23.49s/it]","output_type":"stream"},{"name":"stdout","text":"✓ MOBILENETV2: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  60%|██████    | 3/5 [01:16<00:51, 25.95s/it]","output_type":"stream"},{"name":"stdout","text":"✓ EFFICIENTNETB0: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  80%|████████  | 4/5 [01:29<00:21, 21.01s/it]","output_type":"stream"},{"name":"stdout","text":"✓ VIT: 0 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models: 100%|██████████| 5/5 [02:03<00:00, 24.67s/it]","output_type":"stream"},{"name":"stdout","text":"✓ SWIN: 200 CAMs per class\n\n✓ All CAM overlays saved to: /kaggle/working/cam_visualizations\n\n[6/6] Creating comparison grids...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved comparison grid for CNV\n✓ Saved comparison grid for DME\n✓ Saved comparison grid for DRUSEN\n✓ Saved comparison grid for NORMAL\n\n================================================================================\nSUMMARY: GRAD-CAM ANALYSIS COMPLETE\n================================================================================\n\nModels processed: ['resnet50', 'mobilenetv2', 'efficientnetb0', 'vit', 'swin']\nClasses analyzed: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\nImages per class: 200\nTotal CAM visualizations: 3200\n\nOutput: /kaggle/working/cam_visualizations\nFiles: individual cam_<model>_<class>_<idx>.png + comparison_grid_<class>.png\n\n✓ All CAM visualizations saved successfully!\n================================================================================\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport timm\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"NOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - 4 MODELS (ViT SKIPPED)\")\nprint(\"=\"*80)\n\n# ========== CONFIG ==========\nTEST_DIR = '/kaggle/input/split-dataset/test'\nWEIGHTS_DIR = '/kaggle/input/weight/weights'\nOUTPUT_DIR = '/kaggle/working'\nIMG_SIZE = 224\nNUM_IMAGES_PER_CLASS = 200\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"\\n✓ Device: {DEVICE}\")\nprint(f\"✓ Test dataset: {TEST_DIR}\")\nprint(f\"✓ Weights directory: {WEIGHTS_DIR}\")\nprint(f\"✓ Output directory: {OUTPUT_DIR}\")\n\n# ========== 1. LOAD TEST IMAGES ==========\nprint(\"\\n[1/5] Loading test images...\")\nclass_names = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\ntest_images = {cls: [] for cls in class_names}\n\nfor cls in class_names:\n    cls_path = os.path.join(TEST_DIR, cls)\n    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    sampled = random.sample(images, min(NUM_IMAGES_PER_CLASS, len(images)))\n    test_images[cls] = [os.path.join(cls_path, img) for img in sampled]\n\nfor cls in class_names:\n    print(f\"  {cls}: {len(test_images[cls])}\")\ntotal_images = sum(len(v) for v in test_images.values())\nprint(f\"✓ Total test images: {total_images}\")\n\n# ========== 2. PREPROCESS ==========\nprint(\"\\n[2/5] Setting up preprocessing...\")\ndef preprocess_image(img_path):\n    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    if image is None:\n        image = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    image_norm = image_rgb.astype(np.float32) / 255.0\n    image_norm = (image_norm - 0.5) / 0.5\n    img_tensor = torch.from_numpy(image_norm).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n    img_tensor.requires_grad_(True)\n    return img_tensor, image_rgb\n\nprint(\"✓ Preprocessing ready\")\n\n# ========== 3. CUSTOM CNN CLASSIFIER HEAD ==========\nclass CustomHead(nn.Module):\n    def __init__(self, in_features, num_classes=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n    def forward(self, x):\n        return self.fc(x)\n\n# ========== 4. LOAD MODELS (ViT skipped) ==========\nprint(\"\\n[3/5] Loading trained models (ViT skipped)...\")\nmodel_configs = {\n    'resnet50': {\n        'model_fn': lambda: timm.create_model('resnet50', pretrained=False, num_classes=4),\n        'weight_file': 'resnet50_best_80_10_10.pth',\n        'target_layer': 'layer4.2',\n        'has_custom_head': True,\n        'num_features': 2048,\n    },\n    'mobilenetv2': {\n        'model_fn': lambda: timm.create_model('mobilenetv2_100', pretrained=False, num_classes=4),\n        'weight_file': 'mobilenetv2_best_80_10_10.pth',\n        'target_layer': 'conv_head',\n        'has_custom_head': True,\n        'num_features': 1280,\n    },\n    'efficientnetb0': {\n        'model_fn': lambda: timm.create_model('efficientnet_b0', pretrained=False, num_classes=4),\n        'weight_file': 'efficientnetb0_best_80_10_10.pth',\n        'target_layer': 'conv_head',\n        'has_custom_head': True,\n        'num_features': 1280,\n    },\n    'swin': {\n        'model_fn': lambda: timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4),\n        'weight_file': 'swin_best_80_10_10.pth',\n        'target_layer': 'layers.2',\n        'has_custom_head': False,\n    }\n}\n\nmodels = {}\nfor model_name, cfg in model_configs.items():\n    try:\n        model = cfg['model_fn']()\n        if cfg['has_custom_head']:\n            model.fc = CustomHead(cfg['num_features'], num_classes=4)\n        weight_path = os.path.join(WEIGHTS_DIR, cfg['weight_file'])\n        if os.path.exists(weight_path):\n            model.load_state_dict(torch.load(weight_path, map_location=DEVICE), strict=False)\n            model.to(DEVICE)\n            model.eval()\n            models[model_name] = model\n            print(f\"✓ {model_name.upper()} loaded from {weight_path}\")\n        else:\n            print(f\"⚠ {model_name.upper()} weights not found at {weight_path}\")\n    except Exception as e:\n        print(f\"✗ Error loading {model_name}: {e}\")\n\nprint(f\"\\n✓ Total models loaded: {len(models)}\")\n\n# ========== 5. GradCAM for all 4 models ==========\nclass GradCAM:\n    def __init__(self, model, target_layer_name):\n        self.model = model\n        self.target_layer_name = target_layer_name\n        self.gradients = None\n        self.activations = None\n        self.hooks = []\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def forward_hook(module, inp, out):\n            self.activations = out\n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0]\n        for name, module in self.model.named_modules():\n            if self.target_layer_name in name or name.endswith(self.target_layer_name):\n                self.hooks.append(module.register_forward_hook(forward_hook))\n                self.hooks.append(module.register_full_backward_hook(backward_hook))\n                break\n\n    def generate_cam(self, input_tensor, class_idx=None):\n        output = self.model(input_tensor)\n        if class_idx is None:\n            class_idx = output.argmax(dim=1).item()\n        self.model.zero_grad()\n        score = output[0, class_idx]\n        score.backward(retain_graph=True)\n        grads = self.gradients.detach().cpu().numpy()[0]\n        acts = self.activations.detach().cpu().numpy()[0]\n        weights = grads.mean(axis=(1, 2))\n        cam = np.zeros(acts.shape[1:], dtype=np.float32)\n        for c, w in enumerate(weights):\n            cam += w * acts[c]\n        cam = np.maximum(cam, 0)\n        cam = cam / (cam.max() + 1e-8)\n        return cam, class_idx\n\n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n\nprint(\"✓ Grad-CAM ready\")\n\n# ========== 6. GENERATE CAMs ==========\nprint(\"\\n[5/5] Generating CAMs...\")\n\ncam_output_dir = os.path.join(OUTPUT_DIR, 'cam_visualizations')\nos.makedirs(cam_output_dir, exist_ok=True)\n\nall_cams = {}\n\nif len(models) == 0:\n    print(\"✗ No models loaded. Cannot generate CAMs.\")\nelse:\n    for model_name, model in tqdm(models.items(), desc=\"Processing models\"):\n        target_layer = model_configs[model_name]['target_layer']\n        cam_engine = GradCAM(model, target_layer)\n        all_cams[model_name] = {}\n        for cls_name in class_names:\n            all_cams[model_name][cls_name] = []\n            for img_idx, img_path in enumerate(test_images[cls_name]):\n                try:\n                    img_tensor, img_rgb = preprocess_image(img_path)\n                    cam, pred_class = cam_engine.generate_cam(img_tensor)\n                    cam_resized = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n                    cam_color = cv2.applyColorMap((cam_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n                    cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n                    img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n                    overlay = cv2.addWeighted(img_rgb_uint8, 0.5, cam_color, 0.5, 0)\n                    save_filename = f\"cam_{model_name}_{cls_name}_{img_idx:03d}.png\"\n                    cv2.imwrite(os.path.join(cam_output_dir, save_filename),\n                                cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n                    all_cams[model_name][cls_name].append({\n                        'path': img_path,\n                        'overlay': overlay,\n                        'pred_class': pred_class\n                    })\n                except Exception:\n                    pass\n        cam_engine.remove_hooks()\n        print(f\"✓ {model_name.upper()}: {len(all_cams[model_name][class_names[0]])} CAMs per class\")\n\n    print(f\"\\n✓ All CAM overlays saved to: {cam_output_dir}\")\n\n# ========== 7. CREATE COMPARISON GRIDS ==========\nprint(\"\\n[6/6] Creating comparison grids...\")\nfor cls_name in class_names:\n    fig, axes = plt.subplots(1, len(models) + 1, figsize=(24, 4))\n    first_img_path = test_images[cls_name][0]\n    _, img_rgb = preprocess_image(first_img_path)\n    img_rgb_uint8 = (img_rgb * 255).astype(np.uint8)\n    axes[0].imshow(img_rgb_uint8, cmap='gray')\n    axes[0].set_title(f'{cls_name} - Original', fontsize=11, fontweight='bold')\n    axes[0].axis('off')\n    for i, (model_name, _) in enumerate(models.items()):\n        if len(all_cams[model_name][cls_name]) > 0:\n            overlay = all_cams[model_name][cls_name][0]['overlay']\n            axes[i+1].imshow(overlay)\n            axes[i+1].set_title(model_name.upper(), fontsize=11, fontweight='bold')\n        axes[i+1].axis('off')\n    plt.tight_layout()\n    plt.savefig(os.path.join(cam_output_dir, f'comparison_grid_{cls_name}.png'), dpi=300)\n    plt.close()\n    print(f\"✓ Saved comparison grid for {cls_name}\")\n\n# ========== 8. SUMMARY ==========\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: GRAD-CAM ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n\nif len(models) > 0:\n    print(f\"\\nModels processed: {list(models.keys())}\")\n    print(f\"Classes analyzed: {class_names}\")\n    print(f\"Images per class: {NUM_IMAGES_PER_CLASS}\")\n    total_cams = sum(len(all_cams[m][c]) for m in models.keys() for c in class_names)\n    print(f\"Total CAM visualizations: {total_cams}\")\n    print(f\"\\nOutput: {cam_output_dir}\")\n    print(f\"Files: individual cam_<model>_<class>_<idx>.png + comparison_grid_<class>.png\")\n    print(\"\\n✓ All CAM visualizations saved successfully!\")\nelse:\n    print(\"\\n✗ No models loaded. Check weights.\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:37:26.138815Z","iopub.execute_input":"2025-11-26T22:37:26.139317Z","iopub.status.idle":"2025-11-26T22:39:20.192853Z","shell.execute_reply.started":"2025-11-26T22:37:26.139295Z","shell.execute_reply":"2025-11-26T22:39:20.192228Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nNOTEBOOK 6: CLASS ACTIVATION MAPS (CAM) - 4 MODELS (ViT SKIPPED)\n================================================================================\n\n✓ Device: cuda\n✓ Test dataset: /kaggle/input/split-dataset/test\n✓ Weights directory: /kaggle/input/weight/weights\n✓ Output directory: /kaggle/working\n\n[1/5] Loading test images...\n  CNV: 200\n  DME: 200\n  DRUSEN: 200\n  NORMAL: 200\n✓ Total test images: 800\n\n[2/5] Setting up preprocessing...\n✓ Preprocessing ready\n\n[3/5] Loading trained models (ViT skipped)...\n✓ RESNET50 loaded from /kaggle/input/weight/weights/resnet50_best_80_10_10.pth\n✓ MOBILENETV2 loaded from /kaggle/input/weight/weights/mobilenetv2_best_80_10_10.pth\n✓ EFFICIENTNETB0 loaded from /kaggle/input/weight/weights/efficientnetb0_best_80_10_10.pth\n✓ SWIN loaded from /kaggle/input/weight/weights/swin_best_80_10_10.pth\n\n✓ Total models loaded: 4\n✓ Grad-CAM ready\n\n[5/5] Generating CAMs...\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  25%|██▌       | 1/4 [00:24<01:14, 24.73s/it]","output_type":"stream"},{"name":"stdout","text":"✓ RESNET50: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  50%|█████     | 2/4 [00:46<00:46, 23.19s/it]","output_type":"stream"},{"name":"stdout","text":"✓ MOBILENETV2: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models:  75%|███████▌  | 3/4 [01:15<00:25, 25.74s/it]","output_type":"stream"},{"name":"stdout","text":"✓ EFFICIENTNETB0: 200 CAMs per class\n","output_type":"stream"},{"name":"stderr","text":"Processing models: 100%|██████████| 4/4 [01:49<00:00, 27.29s/it]","output_type":"stream"},{"name":"stdout","text":"✓ SWIN: 200 CAMs per class\n\n✓ All CAM overlays saved to: /kaggle/working/cam_visualizations\n\n[6/6] Creating comparison grids...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"✓ Saved comparison grid for CNV\n✓ Saved comparison grid for DME\n✓ Saved comparison grid for DRUSEN\n✓ Saved comparison grid for NORMAL\n\n================================================================================\nSUMMARY: GRAD-CAM ANALYSIS COMPLETE\n================================================================================\n\nModels processed: ['resnet50', 'mobilenetv2', 'efficientnetb0', 'swin']\nClasses analyzed: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\nImages per class: 200\nTotal CAM visualizations: 3200\n\nOutput: /kaggle/working/cam_visualizations\nFiles: individual cam_<model>_<class>_<idx>.png + comparison_grid_<class>.png\n\n✓ All CAM visualizations saved successfully!\n================================================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import zipfile\n\nzip_path = os.path.join(OUTPUT_DIR, 'cam_visualizations.zip')\n\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(os.path.join(OUTPUT_DIR, 'cam_visualizations')):\n        for file in files:\n            if file.endswith('.png'):\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, OUTPUT_DIR)\n                zipf.write(file_path, arcname)\n\nprint(f\"\\n✓ ZIP archive created with all CAM images: {zip_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:43:22.998752Z","iopub.execute_input":"2025-11-26T22:43:22.999280Z","iopub.status.idle":"2025-11-26T22:43:27.888342Z","shell.execute_reply.started":"2025-11-26T22:43:22.999255Z","shell.execute_reply":"2025-11-26T22:43:27.887721Z"}},"outputs":[{"name":"stdout","text":"\n✓ ZIP archive created with all CAM images: /kaggle/working/cam_visualizations.zip\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# ========== CONFIG ==========\nTEST_DIR = '/kaggle/input/split-dataset/test'\nOUTPUT_DIR = '/kaggle/working/simple_processing'\nIMG_SIZE = 224\nNUM_IMAGES_PER_CLASS = 50  # reduce if needed\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(\"Simple image processing on OCT test images\")\n\n# ========== 1. COLLECT SAMPLE IMAGES ==========\nclass_names = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\ntest_images = {cls: [] for cls in class_names}\n\nfor cls in class_names:\n    cls_path = os.path.join(TEST_DIR, cls)\n    images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n    images = images[:NUM_IMAGES_PER_CLASS]\n    test_images[cls] = [os.path.join(cls_path, img) for img in images]\n    print(f\"{cls}: {len(test_images[cls])} images\")\n\n# ========== 2. DEFINE PROCESSING FUNCTIONS ==========\n\ndef apply_clahe(gray):\n    # Contrast Limited Adaptive Histogram Equalization\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    return clahe.apply(gray)\n\ndef apply_edges(gray):\n    # Canny edge detection\n    return cv2.Canny(gray, 50, 150)\n\ndef apply_morphology(gray):\n    # Morphological opening (remove small bright noise)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    opened = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)\n    # Top-hat to enhance bright structures if you want\n    tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, kernel)\n    return opened, tophat\n\n# ========== 3. PROCESS AND SAVE ==========\nfor cls in tqdm(class_names, desc=\"Processing classes\"):\n    cls_out_dir = os.path.join(OUTPUT_DIR, cls)\n    os.makedirs(cls_out_dir, exist_ok=True)\n\n    for img_idx, img_path in enumerate(test_images[cls]):\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        if img is None:\n            continue\n        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n\n        # Original\n        orig = img\n\n        # CLAHE\n        clahe_img = apply_clahe(orig)\n\n        # Edges\n        edges = apply_edges(orig)\n\n        # Morphology\n        opened, tophat = apply_morphology(orig)\n\n        # Stack for quick visualization grid (optional)\n        grid = np.zeros((IMG_SIZE * 2, IMG_SIZE * 2), dtype=np.uint8)\n        grid[0:IMG_SIZE, 0:IMG_SIZE] = orig\n        grid[0:IMG_SIZE, IMG_SIZE:2*IMG_SIZE] = clahe_img\n        grid[IMG_SIZE:2*IMG_SIZE, 0:IMG_SIZE] = edges\n        grid[IMG_SIZE:2*IMG_SIZE, IMG_SIZE:2*IMG_SIZE] = tophat\n\n        base = f\"{cls}_{img_idx:03d}\"\n        cv2.imwrite(os.path.join(cls_out_dir, base + \"_orig.png\"), orig)\n        cv2.imwrite(os.path.join(cls_out_dir, base + \"_clahe.png\"), clahe_img)\n        cv2.imwrite(os.path.join(cls_out_dir, base + \"_edges.png\"), edges)\n        cv2.imwrite(os.path.join(cls_out_dir, base + \"_opened.png\"), opened)\n        cv2.imwrite(os.path.join(cls_out_dir, base + \"_tophat.png\"), tophat)\n        cv2.imwrite(os.path.join(cls_out_dir, base + \"_grid.png\"), grid)\n\nprint(f\"\\n✓ Simple processing outputs saved under: {OUTPUT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:51:11.623217Z","iopub.execute_input":"2025-11-26T22:51:11.623882Z","iopub.status.idle":"2025-11-26T22:51:13.219427Z","shell.execute_reply.started":"2025-11-26T22:51:11.623855Z","shell.execute_reply":"2025-11-26T22:51:13.218753Z"}},"outputs":[{"name":"stdout","text":"Simple image processing on OCT test images\nCNV: 50 images\nDME: 50 images\nDRUSEN: 50 images\nNORMAL: 50 images\n","output_type":"stream"},{"name":"stderr","text":"Processing classes: 100%|██████████| 4/4 [00:01<00:00,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"\n✓ Simple processing outputs saved under: /kaggle/working/simple_processing\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import zipfile\n\nzip_path = os.path.join('/kaggle/working', 'simple_processing.zip')\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(OUTPUT_DIR):\n        for f in files:\n            if f.endswith('.png'):\n                fp = os.path.join(root, f)\n                zf.write(fp, os.path.relpath(fp, '/kaggle/working'))\n\nprint(f\"✓ ZIP created: {zip_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T22:51:43.137001Z","iopub.execute_input":"2025-11-26T22:51:43.137738Z","iopub.status.idle":"2025-11-26T22:51:43.521175Z","shell.execute_reply.started":"2025-11-26T22:51:43.137708Z","shell.execute_reply":"2025-11-26T22:51:43.520376Z"}},"outputs":[{"name":"stdout","text":"✓ ZIP created: /kaggle/working/simple_processing.zip\n","output_type":"stream"}],"execution_count":19}]}